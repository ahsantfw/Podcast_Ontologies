# ğŸ§  Knowledge Graph Extraction Plan

## Overview
Extract structured knowledge (concepts, relationships, quotes) from chunked transcripts and store in Neo4j with full provenance tracking.

---

## ğŸ“‹ Requirements (from Project Plan)

### Required Concept Types
- **Concept** - Abstract ideas, theories, frameworks
- **Practice** - Actions, methods, techniques
- **CognitiveState/MindState** - Mental states, emotions, cognitive patterns
- **BehavioralPattern** - Recurring behaviors, habits
- **Principle/Framework** - Guiding principles, conceptual frameworks
- **Outcome/Effect** - Results, consequences, effects
- **Causality** - Cause-effect relationships (as concepts)
- **Person** - Named individuals
- **Place** - Locations, geographical entities
- **Organization** - Companies, institutions, groups
- **Event** - Specific occurrences, happenings

### Required Relationship Types
- **CAUSES** - Source causes target
- **INFLUENCES** - Source influences target
- **OPTIMIZES** - Source optimizes target
- **ENABLES** - Source enables target
- **REDUCES** - Source reduces target
- **LEADS_TO** - Source leads to target
- **REQUIRES** - Source requires target
- **RELATES_TO** - General relationship
- **IS_PART_OF** - Part-whole relationship

### Must Extract
1. **Key Concepts** - All concept types listed above
2. **Relationships** - Between concepts with types
3. **Important Quotes** - Key statements with exact text
4. **Speaker Context** - Who said what
5. **Cross-episode Links** - Connections across episodes

### Provenance Requirements
Every extracted entity must include:
- `source_path` - File path
- `episode_id` - Episode identifier
- `start_char` / `end_char` - Character offsets in source
- `speaker` - Who said it
- `timestamp` - When it was said (if available)
- `chunk_index` - Which chunk it came from

---

## ğŸ—ï¸ Architecture

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              KG Extraction Pipeline                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  1. Chunk Reader                                        â”‚
â”‚     - Read chunks from Qdrant or chunk files            â”‚
â”‚     - Filter by workspace_id                            â”‚
â”‚                                                          â”‚
â”‚  2. LLM Extractor                                       â”‚
â”‚     - GPT-4o for structured extraction                  â”‚
â”‚     - JSON schema for deterministic output              â”‚
â”‚     - Batch processing for efficiency                   â”‚
â”‚                                                          â”‚
â”‚  3. Entity Normalizer                                   â”‚
â”‚     - Normalize concept names (lowercase, dedupe)       â”‚
â”‚     - Link to existing nodes                            â”‚
â”‚     - Handle aliases/synonyms                          â”‚
â”‚                                                          â”‚
â”‚  4. Neo4j Writer                                        â”‚
â”‚     - Create/update nodes                              â”‚
â”‚     - Create relationships                             â”‚
â”‚     - Attach provenance metadata                       â”‚
â”‚     - Handle duplicates                                â”‚
â”‚                                                          â”‚
â”‚  5. Quote Extractor                                     â”‚
â”‚     - Extract important quotes                         â”‚
â”‚     - Link quotes to concepts/speakers                 â”‚
â”‚     - Store with timestamps                            â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Implementation Plan

### Phase 1: Neo4j Setup & Schema Design

**Files:**
- `core_engine/kg/neo4j_client.py` - Neo4j connection & utilities
- `core_engine/kg/schema.py` - Node/relationship schemas

**Tasks:**
1. âœ… Set up Neo4j connection (local or cloud)
2. âœ… Define node labels and properties
3. âœ… Define relationship types and properties
4. âœ… Create indexes for performance
5. âœ… Create constraints (unique node IDs)

**Neo4j Schema:**

```cypher
// Node Labels
(:Concept {id, name, type, description, ...})
(:Person {id, name, ...})
(:Place {id, name, ...})
(:Organization {id, name, ...})
(:Event {id, name, ...})
(:Quote {id, text, speaker, timestamp, ...})

// Relationships
(:Concept)-[:CAUSES {confidence, ...}]->(:Concept)
(:Concept)-[:INFLUENCES {confidence, ...}]->(:Concept)
(:Concept)-[:OPTIMIZES {confidence, ...}]->(:Concept)
(:Concept)-[:ENABLES {confidence, ...}]->(:Concept)
(:Concept)-[:REDUCES {confidence, ...}]->(:Concept)
(:Concept)-[:LEADS_TO {confidence, ...}]->(:Concept)
(:Concept)-[:REQUIRES {confidence, ...}]->(:Concept)
(:Concept)-[:RELATES_TO {confidence, ...}]->(:Concept)
(:Concept)-[:IS_PART_OF {confidence, ...}]->(:Concept)

(:Person)-[:SAID {timestamp, ...}]->(:Quote)
(:Quote)-[:ABOUT {confidence, ...}]->(:Concept)
(:Concept)-[:MENTIONED_IN {episode_id, ...}]->(:Episode)
```

**Provenance Properties (on all nodes/relationships):**
- `source_path` - String
- `episode_id` - String
- `start_char` - Integer
- `end_char` - Integer
- `speaker` - String (nullable)
- `timestamp` - String (nullable)
- `chunk_index` - Integer
- `workspace_id` - String
- `extracted_at` - DateTime

---

### Phase 2: LLM Extraction Module

**Files:**
- `core_engine/kg/extractor.py` - Main extraction logic
- `core_engine/kg/prompts.py` - LLM prompts
- `core_engine/kg/schemas.py` - JSON schemas for extraction

**Tasks:**
1. âœ… Design extraction prompt with examples
2. âœ… Define JSON schema for structured output
3. âœ… Implement batch extraction (multiple chunks per call)
4. âœ… Handle extraction errors gracefully
5. âœ… Add retry logic for API failures

**Extraction Strategy:**
- Process chunks in batches (5-10 chunks per LLM call)
- Use structured output (JSON mode) for deterministic results
- Extract concepts, relationships, and quotes in one pass
- Include confidence scores for human review

**Prompt Structure:**
```
You are extracting structured knowledge from podcast transcripts.

Extract:
1. Concepts (with types: Concept, Practice, CognitiveState, etc.)
2. Relationships between concepts (CAUSES, INFLUENCES, etc.)
3. Important quotes (memorable statements)

For each entity, include:
- Exact text span from the transcript
- Speaker (if mentioned)
- Timestamp (if available)
- Confidence level

Output JSON matching this schema: {...}
```

---

### Phase 3: Entity Normalization

**Files:**
- `core_engine/kg/normalizer.py` - Normalization logic

**Tasks:**
1. âœ… Normalize concept names (lowercase, trim, dedupe)
2. âœ… Link to existing nodes in Neo4j
3. âœ… Handle aliases and synonyms
4. âœ… Merge duplicate concepts

**Normalization Rules:**
- Lowercase all concept names
- Remove extra whitespace
- Handle variations: "meditation" = "Meditation" = "MEDITATION"
- Link aliases: "RR" = "Rick Rubin" (if in context)

---

### Phase 4: Neo4j Writer

**Files:**
- `core_engine/kg/writer.py` - Neo4j write operations

**Tasks:**
1. âœ… Create/update nodes with MERGE
2. âœ… Create relationships with MERGE
3. âœ… Attach provenance metadata
4. âœ… Handle duplicates (update vs. create)
5. âœ… Batch writes for performance

**Write Strategy:**
- Use MERGE to avoid duplicates
- Update provenance arrays (multiple sources per concept)
- Use transactions for atomicity
- Batch writes (100-500 operations per transaction)

---

### Phase 5: Quote Extraction

**Files:**
- `core_engine/kg/quotes.py` - Quote extraction logic

**Tasks:**
1. âœ… Extract memorable/important quotes
2. âœ… Link quotes to concepts
3. âœ… Link quotes to speakers
4. âœ… Store timestamps
5. âœ… Store character offsets

**Quote Criteria:**
- Memorable statements
- Key insights
- Definitions
- Principles
- Important claims

---

### Phase 6: Cross-Episode Linking âœ… **COMPLETE**

**Files:**
- `core_engine/kg/cross_episode.py` - Cross-episode analysis
- `analyze_cross_episode.py` - Standalone analysis script

**Tasks:**
1. âœ… Detect concepts mentioned in multiple episodes
2. âœ… Create cross-episode relationships
3. âœ… Identify recurring themes
4. âœ… Build episode-to-episode links
5. âœ… Find co-occurring concepts
6. âœ… Generate cross-episode statistics

**Strategy:**
- After all episodes processed, analyze for duplicates
- Create CROSS_EPISODE relationships between co-occurring concepts
- Aggregate provenance from multiple episodes
- Identify recurring themes and patterns

**Usage:**
```bash
# Run cross-episode analysis
python analyze_cross_episode.py

# Or use programmatically:
from core_engine.kg import CrossEpisodeLinker, get_neo4j_client

client = get_neo4j_client(workspace_id="production")
linker = CrossEpisodeLinker(client, workspace_id="production")

# Find concepts in multiple episodes
concepts = linker.find_cross_episode_concepts(min_episodes=2)

# Create CROSS_EPISODE links
result = linker.create_cross_episode_links(
    min_episodes=2,
    min_co_occurrences=2,
    min_confidence=0.5
)
```

---

## ğŸ”„ Processing Pipeline

### Step-by-Step Flow

```
1. Load Chunks
   â†“
2. Batch Chunks (5-10 per batch)
   â†“
3. LLM Extraction (structured JSON)
   â†“
4. Normalize Entities
   â†“
5. Write to Neo4j (MERGE nodes/relationships)
   â†“
6. Extract Quotes
   â†“
7. Link Quotes to Concepts/Speakers
   â†“
8. Cross-Episode Analysis (after all episodes)
```

---

## ğŸ“Š Data Flow

```
Chunks (from Qdrant/chunking)
    â†“
LLM Extractor (GPT-4o)
    â†“
Structured JSON (concepts, relationships, quotes)
    â†“
Normalizer (dedupe, link)
    â†“
Neo4j Writer (MERGE operations)
    â†“
Knowledge Graph (with provenance)
```

---

## ğŸ¯ Success Criteria

1. âœ… Extract all required concept types
2. âœ… Extract all required relationship types
3. âœ… Store full provenance (source, speaker, timestamp, offsets)
4. âœ… Handle duplicates correctly
5. âœ… Process 100+ episodes efficiently
6. âœ… Extract quotes with speaker context
7. âœ… Enable cross-episode queries

---

## ğŸš€ Next Steps

1. **Set up Neo4j** (local Docker or cloud)
2. **Implement schema** (nodes, relationships, indexes)
3. **Build extractor** (LLM + structured output)
4. **Test on single file** (verify extraction quality)
5. **Scale to all files** (batch processing)
6. **Add cross-episode linking** (post-processing)

---

## ğŸ“ File Structure

```
core_engine/kg/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ KG_EXTRACTION_PLAN.md (this file)
â”œâ”€â”€ neo4j_client.py      # Neo4j connection
â”œâ”€â”€ schema.py            # Schema definitions
â”œâ”€â”€ extractor.py         # LLM extraction logic
â”œâ”€â”€ prompts.py           # LLM prompts
â”œâ”€â”€ schemas.py           # JSON schemas
â”œâ”€â”€ normalizer.py        # Entity normalization
â”œâ”€â”€ writer.py            # Neo4j write operations
â”œâ”€â”€ quotes.py            # Quote extraction
â””â”€â”€ cross_episode.py     # Cross-episode linking
```

---

## ğŸ”§ Configuration

**Environment Variables:**
- `NEO4J_URI` - Neo4j connection URI
- `NEO4J_USER` - Username
- `NEO4J_PASSWORD` - Password
- `OPENAI_API_KEY` - For LLM extraction
- `KG_EXTRACTION_BATCH_SIZE` - Chunks per LLM call (default: 5)
- `KG_CONFIDENCE_THRESHOLD` - Min confidence for extraction (default: 0.7)

---

## ğŸ“ Notes

- **Deterministic Output**: Use structured JSON output from LLM
- **Provenance First**: Every entity must have source tracking
- **Incremental Processing**: Support adding new episodes without reprocessing
- **Error Handling**: Gracefully handle LLM failures, retry logic
- **Performance**: Batch operations, use transactions efficiently

