"""
PODCAST INTELLIGENCE AGENT v3 - FULLY LLM-DRIVEN

A truly intelligent agent that:
1. Uses LLM for ALL responses (no hard-coded text)
2. Dynamically queries Neo4j based on user intent
3. Properly extracts and displays sources with all metadata
4. Handles complex queries intelligently

ARCHITECTURE:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     LLM BRAIN (Decides Everything)                       ‚îÇ
‚îÇ  - Understands user intent semantically                                  ‚îÇ
‚îÇ  - Generates Cypher queries dynamically                                  ‚îÇ
‚îÇ  - Synthesizes natural responses                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚ñº                       ‚ñº                       ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   RAG Tool    ‚îÇ      ‚îÇ   KG Tool     ‚îÇ      ‚îÇ   Memory      ‚îÇ
    ‚îÇ   (Qdrant)    ‚îÇ      ‚îÇ   (Neo4j)     ‚îÇ      ‚îÇ   (Context)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""

from __future__ import annotations

from typing import Optional, Dict, Any, List
from dataclasses import dataclass, field
import json
import os
import re
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

from core_engine.logging import get_logger
from core_engine.reasoning.style_config import STYLE_INSTRUCTIONS, DEFAULT_STYLE
from core_engine.reasoning.tone_config import TONE_INSTRUCTIONS, DEFAULT_TONE

load_dotenv()


@dataclass
class AgentResponse:
    """Response from the agent."""
    answer: str
    tools_used: List[str] = field(default_factory=list)
    sources: List[Dict[str, Any]] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


class PodcastAgent:
    """
    Fully LLM-Driven Podcast Intelligence Agent.
    
    NO hard-coded responses - everything is generated by LLM.
    """
    
    # SYSTEM PURPOSE - Core mission and capabilities
    SYSTEM_PURPOSE = """
    PODCAST INTELLIGENCE ASSISTANT - System Purpose & Capabilities
    
    MISSION:
    Help users explore insights from podcast transcripts about philosophy, creativity, 
    coaching, and personal development through intelligent querying of a knowledge graph.
    
    DOMAIN:
    - üß† Philosophy: Deep thinking, wisdom, mental models, meaning of life
    - üé® Creativity: Innovation, artistic process, creative practices, inspiration
    - üèÜ Coaching: Leadership, performance, team dynamics, personal growth
    - üßò Personal Development: Mindfulness, meditation, self-awareness, well-being
    
    CAPABILITIES:
    ‚úÖ Answer questions about concepts, people, and ideas from podcasts
    ‚úÖ Find relationships between different concepts (Practice ‚Üí Outcome)
    ‚úÖ Retrieve relevant quotes with full source attribution (episode, speaker, timestamp)
    ‚úÖ Discover patterns across multiple episodes (cross-episode analysis)
    ‚úÖ Multi-hop reasoning (X ‚Üí Y ‚Üí Z connections)
    ‚úÖ Speaker-anchored queries ("What did Phil Jackson say about...?")
    
    WHAT I KNOW:
    - Structured knowledge graph (Neo4j) with concepts, practices, outcomes, relationships
    - Vector search (Qdrant) for semantic transcript search
    - Full conversation history and context
    - Episode metadata (episode_id, speaker, timestamp)
    
    WHAT I DON'T KNOW:
    ‚ùå Current events, news, real-time information
    ‚ùå General world knowledge outside podcast corpus
    ‚ùå Weather, sports scores, stock prices
    ‚ùå How to write code or debug programs
    ‚ùå Recipes, geography facts, general trivia
    
    QUERY TYPES I HANDLE:
    1. KNOWLEDGE_QUERY: Questions about podcast content
       - "What is creativity?"
       - "Who is Phil Jackson?"
       - "What practices help with anxiety?"
       - "What did Rick Rubin say about the creative process?"
    
    2. RELATIONSHIP_QUERY: How concepts connect
       - "How does meditation relate to focus?"
       - "What practices optimize creativity?"
       - "What leads to better decision-making?"
    
    3. PATTERN_QUERY: Cross-episode patterns
       - "What concepts appear across multiple episodes?"
       - "What are shared principles about creativity?"
       - "What do successful people have in common?"
    
    4. SOURCE_QUERY: Finding specific quotes or sources
       - "Find quotes about meditation"
       - "What did speaker X say about topic Y?"
       - "Show me sources about creativity"
    
    INTELLIGENT QUERY PROCESSING:
    - Understand user intent in context of system purpose
    - Route to appropriate tool (RAG, KG, or direct response)
    - Synthesize multi-source answers with full attribution
    - Maintain conversation context for follow-up questions
    """
    
    # Out of scope patterns - these get refused WITHOUT searching RAG/KG
    OUT_OF_SCOPE_PATTERNS = [
        r"\b(weather|temperature|forecast)\b",
        r"\b(president|prime minister|politician|election|vote|government)\b",
        r"\b(stock|crypto|bitcoin|price|market|trading)\b",
        r"\b(sports? score|game score|who won the game|match result)\b",
        r"\b(write code|program|python|javascript|sql query|debug)\b",
        r"\b(recipe|cook|food|restaurant|menu)\b",
        r"\b(news today|current events|breaking news)\b",
        r"\b(capital of|population of|geography)\b",
        r"\bwho is (?:the )?(?:pm|president|king|queen|leader) of\b",
        r"\b(translate|translation|language)\b",
        r"\b(calculate|math|equation|solve|algebra|geometry|trigonometry|calculus)\b",
        r"\b(\d+\s*[+\-*/]\s*\d+|x\s*[=+\-*/]|solve for x|what is x|value of x)\b",
        r"\b(if\s+\d+.*=\s*\d+.*what is|if\s+.*equation|solve this|math problem)\b",
        r"\b(2\+2|3x|5y|quadratic|polynomial|derivative|integral)\b",
    ]

    def __init__(
        self,
        workspace_id: str = "default",
        model: str = "gpt-4o-mini",
        hybrid_retriever=None,
        neo4j_client=None,
    ):
        self.workspace_id = workspace_id
        self.model = model
        self.hybrid_retriever = hybrid_retriever
        self.neo4j_client = neo4j_client
        self.logger = get_logger("core_engine.reasoning.agent", workspace_id=workspace_id)
        
        # Initialize OpenAI
        try:
            from openai import OpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found")
            self.openai_client = OpenAI(api_key=api_key)
        except Exception as e:
            self.logger.error(f"OpenAI init failed: {e}")
            self.openai_client = None
        
        self.logger.info("agent_v3_initialized", extra={
            "workspace_id": workspace_id,
            "has_rag": hybrid_retriever is not None,
            "has_kg": neo4j_client is not None,
        })

    def _get_style_tone_instructions(self, session_metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate style and tone-specific instructions based on user preferences.
        
        Instructions are loaded from config files:
        - core_engine/reasoning/style_config.py (for styles)
        - core_engine/reasoning/tone_config.py (for tones)
        
        Developers can modify these config files to change prompt instructions.
        """
        style = session_metadata.get("style", DEFAULT_STYLE) if session_metadata else DEFAULT_STYLE
        tone = session_metadata.get("tone", DEFAULT_TONE) if session_metadata else DEFAULT_TONE
        
        # Get instructions from config files
        style_text = STYLE_INSTRUCTIONS.get(style, STYLE_INSTRUCTIONS[DEFAULT_STYLE])
        tone_text = TONE_INSTRUCTIONS.get(tone, TONE_INSTRUCTIONS[DEFAULT_TONE])
        
        return f"""
{style_text}

{tone_text}

Apply these consistently throughout your response."""

    def run(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """
        Run the agent - LLM decides everything.
        """
        if not self.openai_client:
            return AgentResponse(
                answer="I'm having trouble connecting. Please try again.",
                metadata={"error": "OpenAI client not available"}
            )
        
        # Ensure session_metadata exists
        if session_metadata is None:
            session_metadata = {}
        
        # Log session state for debugging
        self.logger.info(f"Session metadata at start: user_name={session_metadata.get('user_name')}, active_entity={session_metadata.get('active_entity')}")
        
        query_lower = query.lower().strip()
        
        # Check for out of scope FIRST (before any LLM calls)
        if self._is_out_of_scope(query_lower):
            self.logger.info(f"Query detected as OUT OF SCOPE: {query[:50]}")
            return self._handle_out_of_scope_llm(query, conversation_history)
        
        # Let LLM decide what to do
        intent = self._classify_intent_llm(query, conversation_history, session_metadata)
        
        self.logger.info(f"LLM classified intent: {intent}", extra={"query": query[:50]})
        
        # Route based on LLM's decision
        if intent == "out_of_scope":
            self.logger.info(f"Query classified as OUT OF SCOPE by LLM: {query[:50]}")
            return self._handle_out_of_scope_llm(query, conversation_history)
        
        # CRITICAL: Check if query is a knowledge question BEFORE routing
        query_lower = query.lower().strip()
        question_patterns = [
            r"^(what|who|how|why|when|where|are|is|can|do|does|did|will|would|should|tell me|explain|describe|list|show|give|find|search)",
            r"\?$",  # Ends with question mark
        ]
        is_question = any(re.search(pattern, query_lower, re.IGNORECASE) for pattern in question_patterns)
        
        # Check for knowledge-seeking phrases
        knowledge_phrases = [
            "issues", "problems", "solutions", "concepts", "ideas", "practices",
            "society", "societal", "main", "translate", "weather", "current", "pm of", "prime minister",
            "rag", "retrieval", "augmented", "generation"
        ]
        has_knowledge_phrase = any(phrase in query_lower for phrase in knowledge_phrases)
        
        # CRITICAL: If it's a question or has knowledge phrases, ALWAYS route to knowledge query
        # This prevents misclassification from bypassing retrieval
        if is_question or has_knowledge_phrase:
            if intent in ["greeting", "conversational"]:
                # This was misclassified - treat as knowledge query
                self.logger.warning(
                    f"Query '{query[:50]}' was classified as {intent} but looks like a knowledge question. Routing to retrieval.",
                    extra={"is_question": is_question, "has_knowledge_phrase": has_knowledge_phrase}
                )
                return self._handle_knowledge_query(query, conversation_history, session_metadata)
        
        if intent == "greeting":
            # CRITICAL: Verify it's a TRUE greeting before allowing
            true_greeting_patterns = ["hi", "hello", "hey", "thanks", "thank you", "bye", "goodbye", "hmm", "ok", "okay"]
            is_true_greeting = (
                query_lower in true_greeting_patterns or 
                query_lower in [p + "!" for p in true_greeting_patterns] or 
                query_lower in [p + "." for p in true_greeting_patterns]
            )
            
            # If it's NOT a true greeting but classified as one, route to knowledge query
            if not is_true_greeting and (is_question or has_knowledge_phrase):
                self.logger.warning(f"Query '{query[:50]}' was classified as greeting but looks like a knowledge question. Routing to retrieval.")
                return self._handle_knowledge_query(query, conversation_history, session_metadata)
            
            # Check if user is introducing themselves
            self._extract_user_info(query, session_metadata)
            return self._handle_with_llm(query, conversation_history, "greeting", session_metadata)
        
        if intent == "conversational":
            # Check if user is sharing info about themselves
            self._extract_user_info(query, session_metadata)
            
            # SAFETY CHECK: If query looks like a knowledge question, route to retrieval instead
            # This prevents misclassification from bypassing retrieval
            if is_question or has_knowledge_phrase:
                # This was misclassified - treat as knowledge query
                self.logger.warning(f"Query '{query[:50]}' was routed to _handle_with_llm but looks like a knowledge question. Routing to retrieval.")
                return self._handle_knowledge_query(query, conversation_history, session_metadata)
            
            return self._handle_with_llm(query, conversation_history, "conversational", session_metadata)
        
        if intent == "system_info":
            return self._handle_with_llm(query, conversation_history, "system_info", session_metadata)
        
        if intent == "user_memory":
            return self._handle_user_memory(query, conversation_history, session_metadata)
        
        if intent == "list_kg":
            return self._handle_list_kg_dynamic(query, conversation_history)
        
        if intent == "kg_query":
            return self._handle_kg_query_dynamic(query, conversation_history, session_metadata)
        
        # Default: knowledge query with RAG + KG
        return self._handle_knowledge_query(query, conversation_history, session_metadata)

    def _is_out_of_scope(self, query_lower: str) -> bool:
        """Check if query is clearly out of scope."""
        for pattern in self.OUT_OF_SCOPE_PATTERNS:
            if re.search(pattern, query_lower, re.IGNORECASE):
                return True
        return False

    def _classify_intent_llm(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Use LLM to classify intent."""
        
        # Build context
        context = ""
        if conversation_history:
            recent = conversation_history[-5:]
            context = "\n".join([f"{m.get('role', 'user')}: {m.get('content', '')[:100]}" for m in recent])
        
        # Check for user memory in session
        user_info = ""
        if session_metadata:
            user_name = session_metadata.get("user_name", "")
            user_facts = session_metadata.get("user_facts", [])
            if user_name or user_facts:
                user_info = f"\nUSER INFO STORED: name={user_name}, facts={user_facts}"
        
        prompt = f"""You are an intelligent intent classifier for a Podcast Intelligence System.

{self.SYSTEM_PURPOSE}

CRITICAL: This system ONLY answers questions about podcast transcripts, knowledge graph content, and topics related to philosophy, creativity, coaching, and personal development. 

STRICT RULES - REJECT IMMEDIATELY:
- Math problems, equations, calculations, algebra, geometry ‚Üí NOT ALLOWED
- General knowledge questions (history, science facts, geography) ‚Üí NOT ALLOWED  
- Current events, news, politics ‚Üí NOT ALLOWED
- Code, programming, technical questions ‚Üí NOT ALLOWED
- Any question NOT related to podcast content or knowledge graph ‚Üí NOT ALLOWED

YOUR TASK: Classify the user query into ONE category based on:
1. The system's purpose and capabilities
2. Whether the query aligns STRICTLY with the domain (philosophy, creativity, coaching, personal development from podcasts)
3. Whether the query requires searching the knowledge base

CATEGORIES:
- greeting: Simple hello/hi/hey (ONLY for greetings, NOT questions)
- conversational: Casual chat, thanks, how are you, reactions (hmm, ok, wow) - ONLY for non-question statements
- system_info: Questions about what this system is/does (capabilities, purpose)
- user_memory: Questions about what the user told you (their name, preferences, things they said)
- list_kg: Requests to list/show ALL concepts, nodes, or relationship types from knowledge graph
- kg_query: Questions about SPECIFIC relationships (e.g., "what is ABOUT", "show ENABLES", "what does X relate to")
- knowledge_query: Questions seeking information from podcast content (concepts, people, practices, quotes) - DEFAULT for any question
- out_of_scope: ANY question NOT about podcast content, knowledge graph, or domain topics (math, general knowledge, etc.)

CRITICAL RULE: If a query is a QUESTION (starts with "what", "who", "how", "why", "when", "where", "are", "is", "can", "do", etc.), it is ALMOST ALWAYS knowledge_query, NOT conversational, UNLESS it's clearly asking about the user's personal info or system capabilities.

INTELLIGENT CLASSIFICATION RULES:

1. If query asks "how many transcripts/episodes/concepts" ‚Üí knowledge_query (needs to query data)
2. If query is about podcast content (people, concepts, practices, ideas, "what practices are associated with X") ‚Üí knowledge_query
3. If query asks about system structure ("list concepts", "show relationships") ‚Üí list_kg
4. If query asks about specific relationship types ("what is ABOUT?", "show ENABLES") ‚Üí kg_query
5. If query is clearly out of scope (weather, news, code) ‚Üí Already filtered, won't reach here
6. Consider the system's domain - if query doesn't relate to philosophy/creativity/coaching/personal development, 
   but is general knowledge, be honest about limitations

7. If query is math, calculation, equation, problem-solving ‚Üí out_of_scope
8. If query asks "how many transcripts/episodes/concepts" ‚Üí knowledge_query (needs to query data)
9. If query is about podcast content (people, concepts, practices, ideas, "what practices are associated with X") ‚Üí knowledge_query
10. If query asks about system structure ("list concepts", "show relationships") ‚Üí list_kg
11. If query asks about specific relationship types ("what is ABOUT?", "show ENABLES") ‚Üí kg_query
12. STRICT: Only classify as knowledge_query/list_kg/kg_query if it's clearly about podcast/knowledge graph content

EXAMPLES:
- "list all concepts" ‚Üí list_kg
- "what are all relationships" ‚Üí list_kg  
- "what is the ABOUT relationship" ‚Üí kg_query (asking about relationship TYPE)
- "show me ENABLES connections" ‚Üí kg_query (asking about relationship TYPE)
- "what practices are most associated with clarity" ‚Üí knowledge_query (asking about CONTENT)
- "what concepts appear across multiple episodes" ‚Üí knowledge_query (asking about CONTENT)
- "what did Phil Jackson say about creativity" ‚Üí knowledge_query (asking about CONTENT)
- "what is creativity" ‚Üí knowledge_query (asking about concept from podcasts)
- "how many transcripts/episodes/concepts" ‚Üí knowledge_query (asking about data/counts)
- "who is the PM of Pakistan" ‚Üí Already filtered (out of scope)

CRITICAL DISTINCTION:
- kg_query: Questions about the STRUCTURE of the knowledge graph (relationship types, graph structure)
- knowledge_query: Questions about the CONTENT in the knowledge graph (practices, concepts, people, quotes, counts)

CONVERSATION CONTEXT:
{context if context else "No prior context"}
{user_info}

USER QUERY: "{query}"

Think intelligently: Does this query align with the system's purpose? What is the user really asking for?
Respond with ONLY the category name (one word):"""

        try:
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an intelligent intent classifier for a Podcast Intelligence System. You understand the system's purpose and capabilities. Classify queries by considering what the user is really asking and whether it aligns with the system's domain. Respond with only the category name."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=20,
            )
            intent = response.choices[0].message.content.strip().lower()
            
            # Validate intent
            valid_intents = {"greeting", "conversational", "system_info", "user_memory", "list_kg", "kg_query", "knowledge_query", "out_of_scope"}
            if intent not in valid_intents:
                # If unclear, default to out_of_scope to be safe
                return "out_of_scope"
            return intent
        except Exception as e:
            self.logger.error(f"Intent classification failed: {e}")
            return "knowledge_query"

    def _handle_with_llm(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        intent_type: str = "general",
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """Handle any intent with LLM - no hard-coded responses."""
        
        # SAFETY CHECK: If this looks like a knowledge question, route to retrieval instead
        # This prevents misclassification from bypassing retrieval
        if intent_type == "conversational":
            question_patterns = [
                r"^(what|who|how|why|when|where|are|is|can|do|does|did|will|would|should|tell me|explain|describe|list|show|give|find|search)",
                r"\?$",  # Ends with question mark
            ]
            query_lower = query.lower().strip()
            is_question = any(re.search(pattern, query_lower, re.IGNORECASE) for pattern in question_patterns)
            
            # Also check for knowledge-seeking phrases
            knowledge_phrases = [
                "issues", "problems", "solutions", "concepts", "ideas", "practices", 
                "what did", "what are", "what is", "who is", "how does", "why does",
                "society", "societal", "main", "translate", "weather", "current", "pm of", "prime minister",
                "rag", "retrieval", "augmented", "generation"
            ]
            has_knowledge_phrase = any(phrase in query_lower for phrase in knowledge_phrases)
            
            if is_question or has_knowledge_phrase:
                # This was misclassified - treat as knowledge query
                self.logger.warning(f"Query '{query[:50]}' was routed to _handle_with_llm but looks like a knowledge question. Routing to retrieval.")
                return self._handle_knowledge_query(query, conversation_history, session_metadata)
        
        # Build context
        context = ""
        if conversation_history:
            recent = conversation_history[-5:]
            context = "\n".join([f"{m.get('role', 'user')}: {m.get('content', '')[:150]}" for m in recent])
        
        # Get style/tone instructions
        style_tone_instructions = self._get_style_tone_instructions(session_metadata)
        
        # Intent-specific instructions with engaging personality
        instructions = {
            "greeting": """You are an enthusiastic Podcast Intelligence Assistant named Sage - a curious explorer of ideas from fascinating podcast conversations.

PERSONALITY:
- Warm, intellectually curious, and genuinely excited about helping
- You love connecting dots between ideas from different thinkers
- You speak like a thoughtful friend who's passionate about learning
- Remember names if shared, and reference past conversations naturally

RESPONSE STYLE:
- Greet warmly and show genuine interest
- Share something intriguing about what you can help with (philosophy, creativity, coaching, personal development)
- Ask an engaging question to invite exploration
- Keep it natural (2-3 sentences)
- You can use 1 emoji to add warmth

Example tone: "Hey! Great to see you. I've been diving into some fascinating conversations about creativity and mindfulness. What's on your mind today?"
""",
            
            "conversational": """You are Sage, a warm and intellectually curious Podcast Intelligence Assistant.

CRITICAL: This intent is ONLY for casual statements, reactions, or personal sharing (thanks, ok, hmm, etc.). 
If the user asks a QUESTION about podcast content, concepts, or knowledge, you MUST NOT answer it here - 
the system will route it to retrieval. Only respond to conversational statements, not knowledge questions.

PERSONALITY:
- Genuinely interested in the person you're talking to
- Responds with empathy and warmth
- Makes connections to relevant podcast insights when natural
- Remembers what users have shared and references it
- Speaks like a thoughtful friend, not a formal assistant

RESPONSE STYLE:
- Be natural and conversational
- Show you're listening and engaged
- If they share something about themselves, acknowledge it genuinely
- Gently invite deeper exploration when appropriate
- Keep responses brief but warm (1-2 sentences)
- DO NOT answer knowledge questions - only respond to conversational statements
""",
            
            "system_info": f"""You are Sage, an enthusiastic Podcast Intelligence Assistant who loves exploring ideas.

{self.SYSTEM_PURPOSE}

PERSONALITY when explaining yourself:
- Be genuinely excited about your capabilities
- Speak like you're sharing something cool with a friend
- Give specific, intriguing examples of questions
- Show your personality - you're curious, thoughtful, and helpful

When explaining:
1. Share your MISSION with enthusiasm
2. Explain what makes you unique (connecting ideas across conversations)
3. Give 2-3 specific, intriguing example questions that spark curiosity
4. Invite them to explore with you

Example tone: "I'm like your personal guide through some of the most fascinating conversations about creativity, philosophy, and human potential. I can help you discover what Phil Jackson thinks about mindfulness in leadership, or how Rick Rubin approaches creativity..."

Use plain text, avoid complex markdown. Be concise but engaging.""",
        }
        
        instruction = instructions.get(intent_type, instructions["conversational"])
        
        prompt = f"""{instruction}

{style_tone_instructions}

{f"CONVERSATION CONTEXT:{chr(10)}{context}" if context else ""}

USER: "{query}"

Respond naturally:"""

        try:
            # Build messages array with conversation history for proper context
            messages = [{"role": "system", "content": instruction}]
            
            # Add conversation history as proper messages (not just text context)
            if conversation_history:
                for msg in conversation_history[-5:]:  # Last 5 messages
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content:
                        messages.append({"role": role, "content": content})
            
            # Add current query
            messages.append({"role": "user", "content": query})
            
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=300,
            )
            return AgentResponse(
                answer=response.choices[0].message.content,
                metadata={"type": intent_type}
            )
        except Exception as e:
            self.logger.error(f"LLM response failed: {e}")
            return AgentResponse(
                answer="I'm here to help! What would you like to know about the podcasts?",
                metadata={"type": intent_type, "error": str(e)}
            )

    def _extract_user_info(
        self,
        query: str,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Extract and store user information from their messages."""
        if session_metadata is None:
            self.logger.warning("Cannot extract user info - session_metadata is None")
            return
        
        query_lower = query.lower()
        
        # Extract name patterns
        name_patterns = [
            r"(?:my name is|i am|i'm|call me|this is)\s+([a-zA-Z]+)",
            r"^([a-zA-Z]+)\s+here$",
        ]
        
        for pattern in name_patterns:
            match = re.search(pattern, query_lower, re.IGNORECASE)
            if match:
                name = match.group(1).strip().title()
                if len(name) > 1 and name.lower() not in {"hi", "hello", "hey", "ok", "yes", "no", "feeling", "not", "great", "good", "bad"}:
                    session_metadata["user_name"] = name
                    self.logger.info(f"STORED USER NAME: {name} in session_metadata")
                    return
        
        # Store other user facts
        fact_patterns = [
            r"i (?:am|work as|like|love|enjoy|prefer)\s+(.+)",
            r"i'm (?:a|an|from|interested in)\s+(.+)",
        ]
        
        for pattern in fact_patterns:
            match = re.search(pattern, query_lower, re.IGNORECASE)
            if match:
                fact = match.group(0).strip()
                if "user_facts" not in session_metadata:
                    session_metadata["user_facts"] = []
                if fact not in session_metadata["user_facts"]:
                    session_metadata["user_facts"].append(fact)
                    self.logger.info(f"STORED USER FACT: {fact}")
                return

    def _handle_user_memory(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """Handle questions about what the user told us."""
        
        # Get stored user info
        user_name = session_metadata.get("user_name", "") if session_metadata else ""
        user_facts = session_metadata.get("user_facts", []) if session_metadata else []
        
        # Build messages array with conversation history
        messages = [
            {"role": "system", "content": f"""You are Sage, a warm and attentive Podcast Intelligence Assistant who remembers what users share.

USER'S STORED INFO:
- Name: {user_name if user_name else "They haven't shared their name yet"}
- Facts they've shared: {user_facts if user_facts else "Nothing stored yet"}

PERSONALITY:
- Warm and genuinely interested in them
- If you remember something, share it naturally like a friend would
- If you don't have the information, be honest but warm - invite them to share
- Never make up information
- Reference the conversation naturally

Example responses:
- If you know their name: "Of course I remember you, [Name]! You mentioned..."
- If you don't know: "I don't think you've mentioned that yet - I'd love to know!"
"""}
        ]
        
        # Add conversation history for context
        if conversation_history:
            for msg in conversation_history[-8:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role in ["user", "assistant"] and content:
                    messages.append({"role": role, "content": content})
        
        messages.append({"role": "user", "content": query})

        try:
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=200,
            )
            return AgentResponse(
                answer=response.choices[0].message.content,
                metadata={"type": "user_memory", "has_name": bool(user_name)}
            )
        except Exception as e:
            self.logger.error(f"User memory response failed: {e}")
            if user_name:
                return AgentResponse(
                    answer=f"Of course! Your name is {user_name}. What else would you like to know?",
                    metadata={"type": "user_memory"}
                )
            return AgentResponse(
                answer="I don't think you've shared that with me yet - I'd love to know!",
                metadata={"type": "user_memory"}
            )

    def _handle_out_of_scope_llm(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
    ) -> AgentResponse:
        """Handle out of scope with LLM - STRICT refusal focused on product domain."""
        try:
            # Build messages array with conversation history
            messages = [
                {"role": "system", "content": """You are a Podcast Intelligence Assistant named Sage. You are STRICTLY focused on your product domain.

YOUR DOMAIN (ONLY):
- Questions about podcast transcripts and their content
- Concepts, people, practices, and ideas from podcasts
- Knowledge graph queries about philosophy, creativity, coaching, personal development
- Relationships between concepts in the knowledge graph

WHAT YOU CANNOT DO:
- Solve math problems, equations, or calculations
- Answer general knowledge questions
- Provide current events or news
- Write code or solve technical problems
- Answer questions outside the podcast/knowledge graph domain

The user asked something outside your expertise. You MUST politely but firmly redirect them to your domain. Be brief and clear that you only answer questions about podcast content and the knowledge graph. Do NOT attempt to answer the question - only redirect.

Example response: "I'm focused on insights from podcast transcripts about philosophy, creativity, coaching, and personal development. I can help you explore concepts, practices, and ideas from those conversations. What would you like to learn about in those areas?"""}
            ]
            
            # Add conversation history for context
            if conversation_history:
                for msg in conversation_history[-3:]:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content:
                        messages.append({"role": role, "content": content})
            
            messages.append({"role": "user", "content": query})
            
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=150,
            )
            return AgentResponse(
                answer=response.choices[0].message.content,
                metadata={"type": "out_of_scope"}
            )
        except:
            return AgentResponse(
                answer="That's outside my expertise. I specialize in insights from podcasts about philosophy, creativity, coaching, and personal development. What would you like to explore in those areas?",
                metadata={"type": "out_of_scope"}
            )

    def _handle_list_kg_dynamic(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
    ) -> AgentResponse:
        """Handle list/explore KG requests dynamically using LLM to generate Cypher."""
        if not self.neo4j_client:
            return AgentResponse(
                answer="Knowledge graph is not available.",
                metadata={"type": "list_kg", "error": "no_kg"}
            )
        
        # Use LLM to understand what user wants to list
        try:
            # First, understand the request
            understand_prompt = f"""The user wants to list/explore the knowledge graph.
            
User query: "{query}"

What do they want to see? Extract:
1. node_type: Which type of nodes? (Concept, Practice, Person, Principle, Outcome, CognitiveState, BehavioralPattern, or "all")
2. limit: How many? (default 20, max 100)
3. filter: Any specific filter? (e.g., names containing something)

Respond with JSON:
{{"node_type": "...", "limit": 20, "filter": null}}"""

            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Extract structured info from the query. Respond with JSON only."},
                    {"role": "user", "content": understand_prompt}
                ],
                temperature=0.1,
                max_tokens=100,
            )
            
            result_text = response.choices[0].message.content.strip()
            if result_text.startswith("```"):
                result_text = result_text.split("```")[1].replace("json", "").strip()
            
            params = json.loads(result_text)
            node_type = params.get("node_type", "all")
            limit = min(params.get("limit", 20), 100)
            
        except Exception as e:
            self.logger.warning(f"Failed to parse list request: {e}")
            node_type = "all"
            limit = 20
        
        # Build and execute Cypher query
        try:
            if node_type.lower() == "all":
                # Get all types with counts
                cypher = """
                MATCH (c)
                WHERE c.workspace_id = $workspace_id
                  AND (c:Concept OR c:Practice OR c:Person OR c:Principle OR c:Outcome OR c:CognitiveState OR c:BehavioralPattern)
                WITH labels(c)[0] as type, collect(c.name)[..$limit] as names, count(*) as total
                RETURN type, names, total
                ORDER BY total DESC
                """
                results = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id, "limit": limit})
                
                # Get relationship types
                rel_cypher = """
                MATCH (a)-[r]->(b)
                WHERE a.workspace_id = $workspace_id
                RETURN DISTINCT type(r) as relationship, count(*) as count
                ORDER BY count DESC
                LIMIT 15
                """
                rel_results = self.neo4j_client.execute_read(rel_cypher, {"workspace_id": self.workspace_id})
                
                # Format response with proper markdown
                answer_parts = ["## üìö Knowledge Graph Contents"]
                answer_parts.append("")  # Empty line for markdown
                total_concepts = 0
                
                if results:
                    answer_parts.append("### Concepts by Category")
                    answer_parts.append("")
                    for row in results:
                        type_name = row.get("type", "Other")
                        names = row.get("names", [])
                        total = row.get("total", 0)
                        total_concepts += total
                        
                        answer_parts.append(f"**{type_name}** ({total} total):")
                        answer_parts.append("")
                        for name in names[:limit]:
                            answer_parts.append(f"- {name}")
                        if total > limit:
                            answer_parts.append(f"- ... and {total - limit} more")
                        answer_parts.append("")
                
                if rel_results:
                    answer_parts.append("### Relationship Types")
                    answer_parts.append("")
                    for row in rel_results:
                        rel = row.get("relationship", "Unknown")
                        count = row.get("count", 0)
                        answer_parts.append(f"- **{rel}** ({count} connections)")
                    answer_parts.append("")
                
                answer_parts.append("---")
                answer_parts.append(f"*Total: {total_concepts} concepts. Ask me about any of these!*")
                
                return AgentResponse(
                    answer="\n".join(answer_parts),
                    tools_used=["neo4j_query"],
                    metadata={"type": "list_kg", "total_concepts": total_concepts}
                )
            else:
                # Get specific type
                cypher = f"""
                MATCH (c:{node_type})
                WHERE c.workspace_id = $workspace_id
                RETURN c.name as name, c.description as description
                ORDER BY c.name
                LIMIT $limit
                """
                results = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id, "limit": limit})
                
                if not results:
                    return AgentResponse(
                        answer=f"No {node_type} nodes found in the knowledge graph.",
                        metadata={"type": "list_kg"}
                    )
                
                # Format response with proper markdown
                answer_parts = [f"## üìö {node_type} Nodes ({len(results)} shown)"]
                answer_parts.append("")
                for r in results:
                    name = r.get("name", "Unknown")
                    desc = r.get("description", "")
                    if desc:
                        answer_parts.append(f"- **{name}**: {desc[:100]}...")
                    else:
                        answer_parts.append(f"- **{name}**")
                
                return AgentResponse(
                    answer="\n".join(answer_parts),
                    tools_used=["neo4j_query"],
                    metadata={"type": "list_kg", "node_type": node_type, "count": len(results)}
                )
                
        except Exception as e:
            self.logger.error(f"KG list query failed: {e}")
            return AgentResponse(
                answer=f"I couldn't query the knowledge graph: {str(e)}",
                metadata={"type": "list_kg", "error": str(e)}
            )

    def _handle_kg_query_dynamic(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """Handle specific KG queries using LLM-generated Cypher."""
        if not self.neo4j_client:
            return AgentResponse(
                answer="Knowledge graph is not available.",
                metadata={"type": "kg_query", "error": "no_kg"}
            )
        
        query_lower = query.lower()
        
        # Check if user is asking about a specific relationship type
        relationship_types = ["about", "relates_to", "influences", "enables", "leads_to", 
                            "causes", "optimizes", "reduces", "is_part_of", "requires", "enhances"]
        
        for rel_type in relationship_types:
            if rel_type in query_lower:
                # User is asking about a specific relationship - use predefined query
                rel_upper = rel_type.upper()
                cypher = f"""
                MATCH (a)-[r:{rel_upper}]->(b)
                WHERE a.workspace_id = $workspace_id
                RETURN a.name as source, type(r) as relationship, b.name as target, 
                       a.description as source_desc, b.description as target_desc
                LIMIT 20
                """
                self.logger.info(f"Using predefined relationship query for: {rel_upper}")
                
                try:
                    results = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id})
                    
                    if not results:
                        return AgentResponse(
                            answer=f"I couldn't find any {rel_upper} relationships in the knowledge graph.",
                            tools_used=["neo4j_query"],
                            metadata={"type": "kg_query", "relationship": rel_upper}
                        )
                    
                    # Format results
                    answer_parts = [f"## üîó {rel_upper} Relationships\n"]
                    answer_parts.append(f"Found {len(results)} examples of {rel_upper} relationships:\n")
                    
                    for r in results:
                        source = r.get("source", "Unknown")
                        target = r.get("target", "Unknown")
                        answer_parts.append(f"- **{source}** ‚Üí {rel_upper} ‚Üí **{target}**")
                    
                    return AgentResponse(
                        answer="\n".join(answer_parts),
                        tools_used=["neo4j_query"],
                        metadata={"type": "kg_query", "relationship": rel_upper, "count": len(results)}
                    )
                except Exception as e:
                    self.logger.error(f"Relationship query failed: {e}")
                    return AgentResponse(
                        answer=f"I couldn't query the {rel_upper} relationships: {str(e)}",
                        metadata={"type": "kg_query", "error": str(e)}
                    )
        
        # Use LLM to generate Cypher for other queries
        try:
            cypher_prompt = f"""Generate a Cypher query for Neo4j based on this user question.

SCHEMA:
- Node types: Concept, Practice, Person, Principle, Outcome, CognitiveState, BehavioralPattern
- All nodes have: name, description, workspace_id
- Relationship types: RELATES_TO, INFLUENCES, ENABLES, LEADS_TO, CAUSES, OPTIMIZES, REDUCES, IS_PART_OF, ABOUT

RULES:
- Always filter by workspace_id = $workspace_id
- Limit results to 20 unless user specifies
- Return useful fields (name, description, relationships)
- For relationship queries, use: MATCH (a)-[r:REL_TYPE]->(b) WHERE a.workspace_id = $workspace_id RETURN a.name, type(r), b.name

User question: "{query}"

Respond with ONLY the Cypher query, no explanation:"""

            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a Cypher query generator. Output only valid Cypher."},
                    {"role": "user", "content": cypher_prompt}
                ],
                temperature=0.1,
                max_tokens=300,
            )
            
            cypher = response.choices[0].message.content.strip()
            if cypher.startswith("```"):
                cypher = cypher.split("```")[1].replace("cypher", "").strip()
            
            self.logger.info(f"Generated Cypher: {cypher}")
            
            # Execute query
            results = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id})
            
            if not results:
                return AgentResponse(
                    answer="I couldn't find any matching results in the knowledge graph.",
                    tools_used=["neo4j_query"],
                    metadata={"type": "kg_query", "cypher": cypher}
                )
            
            # Use LLM to format results
            format_prompt = f"""Format these Neo4j query results into a natural response.

User question: "{query}"
Results: {json.dumps(results[:10], default=str)}

Provide a clear, well-formatted answer:"""

            format_response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Format database results into a natural, helpful response."},
                    {"role": "user", "content": format_prompt}
                ],
                temperature=0.5,
                max_tokens=500,
            )
            
            return AgentResponse(
                answer=format_response.choices[0].message.content,
                tools_used=["neo4j_query"],
                sources=[{"type": "knowledge_graph", "concept": r.get("name", "")} for r in results[:5] if r.get("name")],
                metadata={"type": "kg_query", "result_count": len(results)}
            )
            
        except Exception as e:
            self.logger.error(f"KG query failed: {e}")
            return AgentResponse(
                answer=f"I couldn't query the knowledge graph: {str(e)}",
                metadata={"type": "kg_query", "error": str(e)}
            )

    def _format_coverage_status(self, coverage_info: Dict[str, Any]) -> str:
        """Format coverage status for prompt (avoid f-string issues)."""
        if not coverage_info or coverage_info["all_covered"]:
            return ""
        missing = ", ".join([e.title() for e in coverage_info["missing"]])
        covered = ", ".join([e.title() for e in coverage_info["covered"]])
        return f"COVERAGE STATUS:\n- ‚úÖ HAVE sources for: {covered}\n- ‚ùå MISSING sources for: {missing}\n\n"

    def _extract_mentioned_entities(self, query: str) -> List[str]:
        """Extract specific entities (people, concepts) mentioned in query."""
        # Known people in corpus (expand as needed)
        known_people = {
            "phil jackson", "jerrod carmichael", "i√±√°rritu", "alejandro inarritu", 
            "rick rubin", "robert greene", "tyler cowen", "marlon brando",
            "will smith", "judd apatow", "whitney cummings", "chris pine",
            "joe dispenza", "carnivore aurelius", "andrew huberman", "david whyte"
        }
        
        query_lower = query.lower()
        entities = []
        
        # Check for known people
        for person in known_people:
            if person in query_lower:
                entities.append(person)
        
        # Pattern: "across X, Y, and Z" or "X, Y, and Z"
        # Pattern: "What do X and Y have in common"
        import re
        across_pattern = r"(?:across|from|among|between)\s+((?:\w+(?:\s+\w+)?(?:\s+and\s+|\s*,\s*))+[\w\s]+)"
        common_pattern = r"(?:common|shared|together).*?([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?(?:\s+and\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)+)"
        
        for pattern in [across_pattern, common_pattern]:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                entity_str = match.group(1).lower()
                # Split by "and", ","
                parts = re.split(r'\s+and\s+|,\s*', entity_str)
                for part in parts:
                    part = part.strip()
                    if part and len(part) > 3:  # Valid name length
                        if part not in entities:
                            entities.append(part)
        
        return entities

    def _validate_entity_coverage(
        self, 
        entities: List[str], 
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Validate that we have sources for all mentioned entities."""
        if not entities:
            return {"all_covered": True, "missing": [], "covered": []}
        
        coverage = {entity: False for entity in entities}
        
        # Check RAG results
        for r in rag_results:
            metadata = r.get("metadata", {})
            episode_id = (metadata.get("episode_id") or "").lower()
            speaker = (metadata.get("speaker") or "").lower()
            text = (r.get("text") or "").lower()
            
            for entity in entities:
                entity_parts = entity.split()
                # Check episode_id (e.g., "002_JERROD_CARMICHAEL")
                if any(part in episode_id for part in entity_parts):
                    coverage[entity] = True
                # Check speaker
                if any(part in speaker for part in entity_parts):
                    coverage[entity] = True
                # Check text mentions
                if entity in text:
                    coverage[entity] = True
        
        # Check KG results (Person nodes)
        for r in kg_results:
            concept = (r.get("concept") or "").lower()
            if r.get("type") == "Person":
                for entity in entities:
                    entity_parts = entity.split()
                    if any(part in concept for part in entity_parts):
                        coverage[entity] = True
        
        covered = [e for e, has_coverage in coverage.items() if has_coverage]
        missing = [e for e, has_coverage in coverage.items() if not has_coverage]
        
        return {
            "all_covered": len(missing) == 0,
            "missing": missing,
            "covered": covered,
        }

    def _handle_count_query(
        self,
        query: str,
    ) -> Optional[AgentResponse]:
        """Handle 'how many' questions by actually querying the data."""
        query_lower = query.lower()
        
        # Check for count questions
        count_patterns = [
            r"how many (?:transcripts?|episodes?)",
            r"how many (?:concepts?|practices?|nodes?)",
            r"what is the (?:total|count|number) of (?:transcripts?|episodes?|concepts?)",
            r"number of (?:transcripts?|episodes?|concepts?)",
        ]
        
        import re
        is_count_query = any(re.search(pattern, query_lower) for pattern in count_patterns)
        
        if not is_count_query:
            return None
        
        # Count transcripts/episodes from Qdrant or Neo4j
        episode_count = 0
        concept_count = 0
        
        # Try to count distinct episodes from Neo4j first (more accurate)
        if self.neo4j_client:
            try:
                # Try to get distinct episode_ids from any node property
                cypher_episodes = """
                MATCH (c)
                WHERE c.workspace_id = $workspace_id
                WITH c.episode_ids as episode_ids
                UNWIND COALESCE(episode_ids, []) as episode_id
                RETURN count(DISTINCT episode_id) as distinct_episodes
                """
                result = self.neo4j_client.execute_read(cypher_episodes, {"workspace_id": self.workspace_id})
                if result and result[0].get("distinct_episodes", 0) > 0:
                    episode_count = result[0].get("distinct_episodes", 0)
            except Exception as e:
                self.logger.warning(f"Could not count episodes from Neo4j: {e}")
        
        # Fallback: Estimate from Qdrant
        if episode_count == 0 and self.hybrid_retriever:
            try:
                qdrant = self.hybrid_retriever.qdrant_client
                collection_name = f"{self.workspace_id}_chunks"
                try:
                    collection_info = qdrant.get_collection(collection_name)
                    total_points = collection_info.points_count
                    # Estimate: ~50-100 chunks per episode on average
                    estimated_episodes = total_points // 75 if total_points > 0 else 0
                    episode_count = estimated_episodes
                except:
                    pass
            except Exception as e:
                self.logger.warning(f"Could not count episodes from Qdrant: {e}")
        
        # Count concepts from Neo4j
        if self.neo4j_client:
            try:
                cypher = """
                MATCH (c)
                WHERE c.workspace_id = $workspace_id
                  AND (c:Concept OR c:Practice OR c:Person OR c:Principle OR c:Outcome 
                       OR c:CognitiveState OR c:BehavioralPattern)
                RETURN count(DISTINCT c) as total
                """
                result = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id})
                if result:
                    concept_count = result[0].get("total", 0)
            except Exception as e:
                self.logger.warning(f"Could not count concepts from Neo4j: {e}")
        
        # Generate answer
        answer_parts = []
        if "transcript" in query_lower or "episode" in query_lower:
            if episode_count > 0:
                answer_parts.append(f"I have access to approximately **{episode_count} podcast episodes** in the knowledge base.")
            else:
                answer_parts.append("I'm processing podcast transcripts, but I don't have an exact count available.")
        
        if "concept" in query_lower or "node" in query_lower or "practice" in query_lower:
            if concept_count > 0:
                answer_parts.append(f"The knowledge graph contains **{concept_count} concepts** (including practices, people, principles, outcomes, and cognitive states).")
            else:
                answer_parts.append("I don't have an exact count of concepts at the moment.")
        
        if not answer_parts:
            return None
        
        return AgentResponse(
            answer=" ".join(answer_parts),
            tools_used=["data_query"],
            metadata={"type": "count_query", "episode_count": episode_count, "concept_count": concept_count}
        )

    def _detect_complex_question(self, query: str) -> Dict[str, Any]:
        """
        Detect if a question is complex (multi-part, ambiguous, or requires clarification).
        
        Returns:
            Dict with 'is_complex', 'reason', and optionally 'sub_questions'
        """
        query_lower = query.lower().strip()
        
        # Patterns indicating multi-part questions
        multi_part_patterns = [
            r'\band\b.*\?',  # "What is X and what about Y?"
            r'\balso\b',     # "Also tell me..."
            r'\bfirst\b.*\bthen\b',  # "First X, then Y"
            r'\b(additionally|furthermore|moreover)\b',
            r'\?.*\?',       # Multiple question marks
        ]
        
        # Check for multiple question indicators
        question_count = query.count('?')
        has_multi_parts = question_count > 1 or any(re.search(p, query_lower) for p in multi_part_patterns)
        
        # Check for vague/ambiguous questions
        vague_patterns = [
            r'^(what|how|why|tell me) about (.{1,10})$',  # Very short "what about X"
            r'^(explain|describe|talk about) (.{1,15})$',
        ]
        is_vague = any(re.search(p, query_lower) for p in vague_patterns)
        
        # Check if it's a comparison question
        comparison_patterns = [
            r'\bcompare\b',
            r'\bdifference between\b',
            r'\bsimilar(ities)?\b.*\bbetween\b',
            r'\bvs\.?\b|\bversus\b',
        ]
        is_comparison = any(re.search(p, query_lower) for p in comparison_patterns)
        
        return {
            "is_complex": has_multi_parts or is_comparison,
            "is_vague": is_vague,
            "has_multiple_parts": has_multi_parts,
            "is_comparison": is_comparison,
            "question_count": question_count,
        }

    def _handle_complex_question(
        self,
        query: str,
        complexity_info: Dict[str, Any],
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> Optional[AgentResponse]:
        """
        Handle complex questions by potentially breaking them down or providing structured responses.
        
        Returns AgentResponse if handled, None if should fall through to normal processing.
        """
        # For comparison questions, ensure we search for all entities
        if complexity_info.get("is_comparison"):
            # Don't block - let normal processing handle it with enhanced search
            return None
        
        # For multi-part questions with 2+ question marks, acknowledge and address each part
        if complexity_info.get("question_count", 0) >= 2:
            # Let the LLM handle multi-part questions naturally
            # The improved prompts should handle this
            return None
        
        return None

    def _handle_knowledge_query(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """Handle knowledge queries - use BOTH RAG + KG."""
        tools_used = []
        rag_results = []
        kg_results = []
        
        # Check for complex questions
        complexity_info = self._detect_complex_question(query)
        if complexity_info.get("is_complex"):
            self.logger.info(f"Detected complex question: {complexity_info}")
            complex_response = self._handle_complex_question(
                query, complexity_info, conversation_history, session_metadata
            )
            if complex_response:
                return complex_response
        
        # Check if this is a count query first
        count_response = self._handle_count_query(query)
        if count_response:
            return count_response
        
        # Check if user is sharing info about themselves
        self._extract_user_info(query, session_metadata)
        
        # Extract mentioned entities for coverage validation
        mentioned_entities = self._extract_mentioned_entities(query)
        if mentioned_entities:
            self.logger.info(f"Detected mentioned entities: {mentioned_entities}")
        
        # Resolve pronouns
        resolved_query = self._resolve_pronouns(query, session_metadata)
        
        # PARALLEL RAG + KG Search for better performance
        rag_results = []
        kg_results = []
        
        def _rag_search():
            """RAG search function for parallel execution."""
            if self.hybrid_retriever:
                try:
                    results = self.hybrid_retriever.retrieve(resolved_query, use_vector=True, use_graph=False)
                    self.logger.info(f"RAG returned {len(results)} results")
                    return results, None
                except Exception as e:
                    self.logger.error(f"RAG search failed: {e}")
                    return [], e
            return [], None
        
        def _kg_search():
            """KG search function for parallel execution."""
            if self.neo4j_client:
                try:
                    results = self._search_knowledge_graph(resolved_query)
                    self.logger.info(f"KG returned {len(results)} results")
                    return results, None
                except Exception as e:
                    self.logger.error(f"KG search failed: {e}")
                    return [], e
            return [], None
        
        # Execute RAG and KG searches in parallel
        with ThreadPoolExecutor(max_workers=2) as executor:
            rag_future = executor.submit(_rag_search)
            kg_future = executor.submit(_kg_search)
            
            # Wait for both to complete
            rag_result, rag_error = rag_future.result()
            kg_result, kg_error = kg_future.result()
            
            rag_results = rag_result
            kg_results = kg_result
            
            if rag_results:
                tools_used.append("search_transcripts")
            if kg_results:
                tools_used.append("search_knowledge_graph")
            
            # Log any errors
            if rag_error:
                self.logger.warning(f"RAG search completed with error: {rag_error}")
            if kg_error:
                self.logger.warning(f"KG search completed with error: {kg_error}")
        
        # Validate entity coverage for multi-entity queries
        coverage_info = None
        if mentioned_entities and len(mentioned_entities) > 1:
            coverage_info = self._validate_entity_coverage(mentioned_entities, rag_results, kg_results)
            self.logger.info(f"Entity coverage: {coverage_info}")
            
            # Filter RAG results to ONLY include mentioned entities (strict filtering)
            filtered_rag = []
            for r in rag_results:
                metadata = r.get("metadata", {})
                episode_id = (metadata.get("episode_id") or "").lower()
                speaker = (metadata.get("speaker") or "").lower()
                text = (r.get("text") or "").lower()
                
                # Check if this result is from one of the mentioned entities
                is_from_mentioned = False
                for entity in mentioned_entities:
                    entity_parts = entity.split()
                    # Check episode_id (e.g., "002_JERROD_CARMICHAEL")
                    if any(part in episode_id for part in entity_parts):
                        is_from_mentioned = True
                        break
                    # Check speaker
                    if any(part in speaker for part in entity_parts):
                        is_from_mentioned = True
                        break
                
                if is_from_mentioned:
                    filtered_rag.append(r)
            
            # Use filtered results (only if we have some)
            if filtered_rag:
                self.logger.info(f"Filtered RAG results: {len(rag_results)} ‚Üí {len(filtered_rag)} (only mentioned entities)")
                rag_results = filtered_rag
            else:
                self.logger.warning(f"No RAG results match mentioned entities: {mentioned_entities}")
            
            if not coverage_info["all_covered"]:
                missing = coverage_info["missing"]
                self.logger.warning(f"Missing coverage for entities: {missing}")
        
        # No results
        if not rag_results and not kg_results:
            return AgentResponse(
                answer="I couldn't find information about that in the podcast knowledge base. Could you rephrase or ask about a different topic related to philosophy, creativity, coaching, or personal development?",
                tools_used=tools_used,
                metadata={"type": "no_results"}
            )
        
        # Synthesize answer (pass coverage info for strict validation)
        answer = self._synthesize_answer(
            query, resolved_query, rag_results[:5], kg_results[:10], 
            conversation_history, coverage_info=coverage_info, mentioned_entities=mentioned_entities,
            session_metadata=session_metadata
        )
        
        # Extract sources with full metadata
        sources = self._extract_sources(rag_results[:5], kg_results[:10])
        
        # Log sources for debugging
        self.logger.info(f"Extracted {len(sources)} sources", extra={
            "sample_source": sources[0] if sources else None,
            "rag_sample_metadata": rag_results[0].get("metadata", {}) if rag_results else None
        })
        
        # Update active entity
        self._update_active_entity(query, rag_results, kg_results, session_metadata)
        
        return AgentResponse(
            answer=answer,
            tools_used=tools_used,
            sources=sources,
            metadata={
                "type": "knowledge_query",
                "rag_count": len(rag_results),
                "kg_count": len(kg_results),
            }
        )

    def _resolve_pronouns(self, query: str, session_metadata: Optional[Dict[str, Any]] = None) -> str:
        """Resolve pronouns using active entity."""
        if not session_metadata:
            return query
        
        active_entity = session_metadata.get("active_entity")
        if not active_entity:
            return query
        
        query_lower = query.lower()
        pronouns = ["he", "she", "they", "him", "her", "them", "his", "her", "their"]
        
        for pronoun in pronouns:
            if f" {pronoun} " in f" {query_lower} " or query_lower.startswith(f"{pronoun} "):
                return f"{query} (referring to {active_entity})"
        
        return query

    def _search_knowledge_graph(self, query: str) -> List[Dict[str, Any]]:
        """Search KG for relevant concepts - OPTIMIZED with single query."""
        words = query.lower().split()
        
        # Extract meaningful search terms (filter stop words and short words)
        stop_words = {"what", "are", "is", "the", "a", "an", "that", "which", "who", "how", "when", "where", "why", "does", "do", "did", "this", "that", "about", "for", "with", "from"}
        search_terms = [w for w in words if len(w) > 2 and w not in stop_words]
        
        # Add full query as first term
        if query.lower().strip():
            search_terms = [query.lower().strip()] + search_terms
        
        # Limit to top 3 most relevant terms
        search_terms = search_terms[:3]
        
        if not search_terms:
            return []
        
        # OPTIMIZED: Single query with OR conditions - Fixed ORDER BY to avoid Neo4j error
        cypher = """
        MATCH (c)
        WHERE c.workspace_id = $workspace_id
          AND (
            ANY(term IN $search_terms WHERE toLower(c.name) CONTAINS term)
            OR ANY(term IN $search_terms WHERE toLower(c.description) CONTAINS term)
          )
        OPTIONAL MATCH (c)-[r]->(related)
        WHERE related.workspace_id = $workspace_id
        WITH DISTINCT c, collect(DISTINCT {rel: type(r), target: related.name})[..5] as relationships
        RETURN c.name as concept, 
               labels(c)[0] as type,
               c.description as description,
               relationships,
               CASE 
                 WHEN ANY(term IN $search_terms WHERE toLower(c.name) = term) THEN 1
                 WHEN ANY(term IN $search_terms WHERE toLower(c.name) STARTS WITH term) THEN 2
                 ELSE 3
               END as relevance
        ORDER BY relevance
        LIMIT 10
        """
        
        try:
            # Log search parameters for debugging
            self.logger.info(f"KG search: workspace_id={self.workspace_id}, search_terms={search_terms}")
            
            # Check Neo4j connection
            if not self.neo4j_client:
                self.logger.error("Neo4j client is None - KG search cannot proceed")
                return []
            
            results = self.neo4j_client.execute_read(
                cypher, 
                {"workspace_id": self.workspace_id, "search_terms": search_terms}
            )
            
            if results:
                self.logger.info(f"KG search returned {len(results)} results for terms: {search_terms}")
                # Log first result for debugging
                if len(results) > 0:
                    self.logger.debug(f"First KG result: {results[0].get('concept', 'N/A')}")
                return results
            else:
                self.logger.warning(f"KG search returned 0 results for terms: {search_terms} (workspace_id: {self.workspace_id})")
                return []
                
        except Exception as e:
            self.logger.error(f"KG search failed: {e}", exc_info=True)
            return []

    def _synthesize_answer(
        self,
        query: str,
        resolved_query: str,
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        coverage_info: Optional[Dict[str, Any]] = None,
        mentioned_entities: Optional[List[str]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Synthesize answer from RAG + KG using LLM with strict entity coverage."""
        import os
        
        # HARD STOP: If no results at all, return immediately WITHOUT any LLM call
        # This is the most critical check - prevents any synthesis when RAG=0, KG=0
        rag_count = len(rag_results) if rag_results else 0
        kg_count = len(kg_results) if kg_results else 0
        
        # CRITICAL: Also check if results are empty lists (not just None)
        rag_has_content = rag_count > 0 and any(r.get("text") or r.get("concept") for r in rag_results)
        kg_has_content = kg_count > 0 and any(r.get("concept") or r.get("description") or r.get("text") for r in kg_results)
        
        # REJECT if both are empty OR both have no content
        if (rag_count == 0 and kg_count == 0) or (not rag_has_content and not kg_has_content):
            self.logger.error(
                "synthesis_hard_stop_no_results",
                extra={
                    "context": {
                        "query": query[:50],
                        "rag_count": rag_count,
                        "kg_count": kg_count,
                        "rag_has_content": rag_has_content,
                        "kg_has_content": kg_has_content,
                        "rag_results_type": type(rag_results).__name__,
                        "kg_results_type": type(kg_results).__name__,
                        "message": "HARD STOP: rag_count=0 AND kg_count=0 OR no content in results, returning without LLM call"
                    }
                }
            )
            return "I couldn't find information about that in the podcast knowledge base. Could you rephrase your question or ask about a specific topic related to philosophy, creativity, coaching, or personal development from the podcasts?"
        
        # Log that we're proceeding with synthesis
        self.logger.info(
            "synthesis_proceeding_with_results",
            extra={
                "context": {
                    "query": query[:50],
                    "rag_count": rag_count,
                    "kg_count": kg_count,
                    "will_build_context": True,
                    "will_call_llm": True
                }
            }
        )
        
        # Get style/tone instructions
        style_tone_instructions = self._get_style_tone_instructions(session_metadata)
        
        # Format RAG context with full metadata
        rag_context = ""
        if rag_results:
            rag_parts = []
            for i, r in enumerate(rag_results, 1):
                text = r.get("text", "")[:400]
                metadata = r.get("metadata", {})
                
                # Extract episode info
                episode_id = metadata.get("episode_id", "")
                source_path = metadata.get("source_path", "")
                if not episode_id or episode_id == "unknown":
                    if source_path:
                        filename = os.path.basename(source_path)
                        episode_id = os.path.splitext(filename)[0]
                
                speaker = metadata.get("speaker", "")
                timestamp = metadata.get("timestamp", "")
                
                # Format source line - emphasize speaker and episode for natural citation
                # Parse episode name to be more readable (e.g., "143_TYLER_COWEN_PART_1" -> "Tyler Cowen")
                speaker_display = speaker if speaker and speaker != "Unknown" else ""
                episode_display = ""
                
                if episode_id and episode_id != "unknown":
                    # Extract name from episode_id like "143_TYLER_COWEN_PART_1"
                    parts = episode_id.split("_")
                    if len(parts) >= 2:
                        # Skip number prefix and "PART" suffix
                        name_parts = [p for p in parts[1:] if p.upper() not in ["PART", "1", "2", "3"]]
                        if name_parts:
                            episode_display = " ".join(name_parts).title()
                    if not episode_display:
                        episode_display = episode_id.replace("_", " ")
                
                # Use episode name as speaker if speaker is generic
                if not speaker_display or speaker_display in ["Speaker 1", "Speaker 2", "Speaker"]:
                    speaker_display = episode_display or "Speaker"
                
                source_info = f"--- {speaker_display}"
                if episode_id and episode_id != "unknown":
                    source_info += f" (Episode: {episode_id})"
                if timestamp:
                    source_info += f" at {timestamp}"
                source_info += " ---"
                
                rag_parts.append(f"{source_info}\n\"{text}\"")
            rag_context = "\n\n".join(rag_parts)
        
        # Format KG context
        kg_context = ""
        if kg_results:
            kg_parts = []
            for r in kg_results:
                concept = r.get("concept", "")
                desc = r.get("description", "")
                rels = r.get("relationships", [])
                
                part = f"**{concept}**"
                if desc:
                    part += f": {desc[:200]}"
                if rels:
                    rel_strs = [f"{rel['rel']} ‚Üí {rel['target']}" for rel in rels[:3]]
                    part += f" (Relationships: {', '.join(rel_strs)})"
                kg_parts.append(part)
            kg_context = "\n".join(kg_parts)
        
        # Conversation context
        conv_context = ""
        if conversation_history:
            recent = conversation_history[-5:]
            conv_parts = [f"{m.get('role', 'user')}: {m.get('content', '')[:200]}" for m in recent]
            conv_context = "\n".join(conv_parts)
        
        # Build entity coverage constraint for multi-entity queries
        entity_coverage_constraint = ""
        if mentioned_entities and len(mentioned_entities) > 1:
            mentioned_list = ', '.join([e.title() for e in mentioned_entities])
            
            if coverage_info and not coverage_info["all_covered"]:
                missing = ", ".join([e.title() for e in coverage_info["missing"]])
                covered = ", ".join([e.title() for e in coverage_info["covered"]])
            else:
                missing = ""
                covered = mentioned_list
            
            entity_coverage_constraint = f"""
CRITICAL ENTITY COVERAGE REQUIREMENT (MULTI-ENTITY QUERY):
The user specifically asked about these people: {mentioned_list}

STRICT RULES - YOU MUST FOLLOW:
1. ONLY use sources from: {mentioned_list}
2. DO NOT use sources from ANYONE else (e.g., Judd Apatow, Rick Rubin, Edward Norton, etc.)
3. If you don't have sources for ALL mentioned entities, be explicit: "I don't have sufficient sources from [entity]"
4. DO NOT use proxy speakers, "peers", "similar creatives", or related people as substitutes
5. DO NOT infer shared principles if you don't have direct evidence from ALL mentioned entities

ALLOWED SPEAKERS FOR THIS QUERY: {mentioned_list}
FORBIDDEN: Any speaker NOT in the list above (even if they appear in the sources below)

{self._format_coverage_status(coverage_info) if coverage_info and not coverage_info["all_covered"] else ''}
EXAMPLE HONEST RESPONSE IF MISSING COVERAGE:
"For Phil Jackson and I√±√°rritu, [principle]. However, I don't have sufficient sources from Jerrod Carmichael in the knowledge base to confirm this principle. Therefore, I can only establish partial overlap."

REMEMBER: Even if Judd Apatow or Rick Rubin appear in the sources, DO NOT use them - they are NOT in the query!"""
        
        # CRITICAL: If no sources, don't synthesize - return explicit message
        # Check both context strings AND result lists to be absolutely sure
        has_rag_content = bool(rag_context and rag_context.strip() and rag_context != "No relevant transcripts found.")
        has_kg_content = bool(kg_context and kg_context.strip() and kg_context != "No relevant concepts found.")
        has_rag_results = bool(rag_results and len(rag_results) > 0)
        has_kg_results = bool(kg_results and len(kg_results) > 0)
        
        # CRITICAL: Also check if results have actual content (not just empty dicts)
        if rag_results:
            has_rag_content = has_rag_content or any(
                r.get("text") or r.get("concept") or r.get("description") 
                for r in rag_results if isinstance(r, dict)
            )
        if kg_results:
            has_kg_content = has_kg_content or any(
                r.get("concept") or r.get("description") or r.get("text") or r.get("name")
                for r in kg_results if isinstance(r, dict)
            )
        
        # ABSOLUTE HARD STOP: If no content at all, return immediately
        if not has_rag_content and not has_kg_content and not has_rag_results and not has_kg_results:
            self.logger.error(
                "synthesis_blocked_no_sources_absolute",
                extra={
                    "context": {
                        "query": query[:50],
                        "rag_context_empty": not rag_context or not rag_context.strip(),
                        "kg_context_empty": not kg_context or not kg_context.strip(),
                        "rag_results_count": len(rag_results) if rag_results else 0,
                        "kg_results_count": len(kg_results) if kg_results else 0,
                        "has_rag_content": has_rag_content,
                        "has_kg_content": has_kg_content,
                        "message": "ABSOLUTE HARD STOP: No content in RAG or KG, blocking LLM call"
                    }
                }
            )
            return "I couldn't find information about that in the podcast knowledge base. Could you rephrase your question or ask about a specific topic related to philosophy, creativity, coaching, or personal development from the podcasts?"
        
        # Synthesize with LLM
        system_prompt = f"""You are Sage, a warm and intellectually curious Podcast Intelligence Assistant. You help people explore insights from fascinating podcast conversations.

CRITICAL: You ONLY answer questions about podcast transcripts, knowledge graph content, and topics related to philosophy, creativity, coaching, and personal development. If a question is about math, general knowledge, current events, or anything outside your domain, you MUST politely redirect the user to your domain. Do NOT attempt to answer questions outside your expertise.

MANDATORY SOURCE REQUIREMENT:
- You MUST base your answer ONLY on the TRANSCRIPT SOURCES and KNOWLEDGE GRAPH content provided below
- If the sources don't contain relevant information, you MUST say "I couldn't find information about that in the podcast knowledge base"
- DO NOT use general knowledge or information not in the provided sources
- DO NOT make up information or cite sources that don't exist
{entity_coverage_constraint}

CORE PERSONALITY - BE ENGAGING AND HUMAN:
- You're a thoughtful friend who genuinely loves exploring ideas - show that passion!
- Speak naturally, like you're having a real conversation, not reading a report
- Vary your sentence structure - mix short punchy sentences with longer flowing ones
- Show genuine curiosity and excitement about the insights you're sharing
- Make connections feel organic and interesting, not forced or mechanical
- Use natural transitions: "What's really interesting here is...", "This connects to...", "I love how..."
- If you remember the user's name or previous context, reference it naturally

{style_tone_instructions}

CRITICAL RULES FOR ACCURACY:
1. ONLY cite and reference speakers/episodes that appear in the TRANSCRIPT SOURCES below
2. DO NOT mention or cite anyone not in the provided sources
3. If a speaker is not in the sources, DO NOT reference them - state this explicitly
4. Cite naturally by speaker name - weave it into the narrative (e.g., "Phil Jackson shares this fascinating insight..." or "As Marlon Brando puts it..." or "Rick Rubin describes how...")
5. NEVER use formulaic citations like "[Source 1]" or "[According to Episode X]" - make it feel natural
6. If the sources don't answer the question, explicitly state: "I couldn't find information about that in the podcast knowledge base"

RESPONSE STYLE - MAKE IT ENGAGING:
- Start with something interesting or engaging, not dry facts ("This is fascinating!" or "What's really interesting here is..." or "I love this question because...")
- Tell a story or paint a picture when possible - don't just list facts
- Vary how you present information:
  * Sometimes start with a specific example, then generalize
  * Sometimes start with the big picture, then dive into details
  * Sometimes use a question to engage: "You know what's interesting about this?"
- Weave citations naturally into your narrative - make them feel like part of the story
- Show genuine enthusiasm: "This is such a powerful insight!", "What's amazing is...", "I find this fascinating because..."
- Make connections feel organic: "This connects beautifully to...", "What's interesting is how this relates to...", "You'll notice a pattern here..."
- Use natural language: "tends to", "often", "sometimes", "in many cases" - avoid absolutes unless clearly supported
- End with something engaging: a question, an invitation to explore, or a thoughtful observation

AVOID DRY/MACHINE-LIKE LANGUAGE:
- DON'T start with "According to the sources..." or "The data shows..." (too robotic)
- DON'T use formulaic structures: "First... Second... Third..." (unless listing is truly needed)
- DON'T be overly structured - let it flow naturally
- DON'T use academic language unless style is "academic"
- DON'T list facts without context or story
- DON'T use repetitive sentence structures

MAKE IT FEEL HUMAN:
- Use natural transitions and connectors
- Show personality and genuine interest
- Vary your approach - don't use the same structure every time
- Make it conversational, not report-like
- Let your enthusiasm show through
- Connect ideas in ways that feel organic, not forced

WHAT YOU CANNOT DO:
- Reference speakers not in the provided sources
- Make up quotes or information
- Use "[Source 1]" style citations
- Infer shared principles without direct evidence
- Answer from general knowledge if sources don't contain the answer
- Be dry, robotic, or machine-like

FORMATTING:
- Use natural paragraphs that flow (2-4 sentences typically)
- For lists, use proper markdown with "- " prefix, but make them conversational
- Use **bold** sparingly for emphasis on key concepts
- Keep formatting simple and clean, but let personality shine through"""

        # Build source sections - be explicit about empty sources
        transcript_section = rag_context if has_rag_content else "No relevant transcripts found."
        kg_section = kg_context if has_kg_content else "No relevant concepts found."
        
        # CRITICAL: If both sources are empty, make it VERY clear to LLM
        # AND add a hard stop instruction
        if not has_rag_content and not has_kg_content:
            transcript_section = "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è NO TRANSCRIPT SOURCES AVAILABLE ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n\nCRITICAL: DO NOT ANSWER FROM GENERAL KNOWLEDGE. YOU MUST RETURN THE REJECTION MESSAGE BELOW."
            kg_section = "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è NO KNOWLEDGE GRAPH SOURCES AVAILABLE ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n\nCRITICAL: DO NOT ANSWER FROM GENERAL KNOWLEDGE. YOU MUST RETURN THE REJECTION MESSAGE BELOW."
        
        user_prompt = f"""Question: {query}

TRANSCRIPT SOURCES (cite by speaker name and episode, NOT by source number):
{transcript_section}

KNOWLEDGE GRAPH:
{kg_section}

{f"CONVERSATION CONTEXT:{chr(10)}{conv_context}" if conv_context else ""}

YOUR TASK:
Create an engaging, natural, and human-sounding response that:
- Feels like a real conversation, not a report or list
- Shows genuine interest and enthusiasm
- Weaves citations naturally into the narrative
- Makes connections feel organic and interesting
- Varies sentence structure and presentation style
- Tells a story or paints a picture when possible

CRITICAL RULES - YOU MUST FOLLOW:
1. Base your answer ONLY on the TRANSCRIPT SOURCES and KNOWLEDGE GRAPH content above
2. If the sources say "No relevant transcripts found" or "No relevant concepts found", you MUST respond with: "I couldn't find information about that in the podcast knowledge base. Could you rephrase your question or ask about a specific topic related to philosophy, creativity, coaching, or personal development from the podcasts?"
3. DO NOT use general knowledge, common sense, or information not in the provided sources
4. DO NOT make up information or cite sources that don't exist
5. If you cannot answer from the sources, explicitly state that you couldn't find information
6. Make citations feel natural - weave speaker names into your narrative, don't use formulaic citations

REMEMBER: Your goal is to make the reader feel engaged and interested, not like they're reading a dry report. Show personality, enthusiasm, and genuine curiosity while maintaining strict accuracy. {f"IMPORTANT: Be honest about missing coverage for any entities." if coverage_info and not coverage_info["all_covered"] else ""}"""

        try:
            # ABSOLUTE HARD CHECK: If no sources, return immediately without LLM call
            # This is the FINAL gate before LLM - be extremely aggressive
            if not has_rag_content and not has_kg_content and not has_rag_results and not has_kg_results:
                self.logger.error(
                    "synthesis_blocked_no_sources_absolute_hard_stop",
                    extra={
                        "context": {
                            "query": query[:50],
                            "has_rag_content": has_rag_content,
                            "has_kg_content": has_kg_content,
                            "has_rag_results": has_rag_results,
                            "has_kg_results": has_kg_results,
                            "rag_count": rag_count,
                            "kg_count": kg_count,
                            "message": "ABSOLUTE HARD STOP: No sources, blocking LLM call"
                        }
                    }
                )
                return "I couldn't find information about that in the podcast knowledge base. Could you rephrase your question or ask about a specific topic related to philosophy, creativity, coaching, or personal development from the podcasts?"
            
            # DOUBLE CHECK: If counts are 0, reject even if content checks passed
            if rag_count == 0 and kg_count == 0:
                self.logger.error(
                    "synthesis_blocked_zero_counts_absolute",
                    extra={
                        "context": {
                            "query": query[:50],
                            "rag_count": rag_count,
                            "kg_count": kg_count,
                            "has_rag_content": has_rag_content,
                            "has_kg_content": has_kg_content,
                            "message": "ABSOLUTE HARD STOP: rag_count=0 AND kg_count=0, blocking LLM call"
                        }
                    }
                )
                return "I couldn't find information about that in the podcast knowledge base. Could you rephrase your question or ask about a specific topic related to philosophy, creativity, coaching, or personal development from the podcasts?"
            
            # Build proper messages array with conversation history for follow-up support
            messages = [{"role": "system", "content": system_prompt}]
            
            # Add conversation history as proper messages (enables better follow-up)
            if conversation_history:
                for msg in conversation_history[-5:]:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content:
                        messages.append({"role": role, "content": content})
            
            # Add current query with context
            messages.append({"role": "user", "content": user_prompt})
            
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,  # Increased for more natural, varied responses (was 0.5)
                # max_tokens removed to allow full-length responses
            )
            
            answer = response.choices[0].message.content
            
            # ABSOLUTE FINAL CHECK: If we have no sources but LLM generated an answer, reject it
            if (rag_count == 0 and kg_count == 0) or (not has_rag_content and not has_kg_content):
                # LLM generated answer despite no sources - reject it
                self.logger.error(
                    "synthesis_llm_generated_answer_without_sources",
                    extra={
                        "context": {
                            "query": query[:50],
                            "rag_count": rag_count,
                            "kg_count": kg_count,
                            "has_rag_content": has_rag_content,
                            "has_kg_content": has_kg_content,
                            "answer_length": len(answer),
                            "answer_preview": answer[:200],
                            "message": "LLM generated answer despite no sources - rejecting"
                        }
                    }
                )
                return "I couldn't find information about that in the podcast knowledge base. Could you rephrase your question or ask about a specific topic related to philosophy, creativity, coaching, or personal development from the podcasts?"
            
            # FINAL SAFETY CHECK: If answer doesn't mention "couldn't find" but we have no sources, force the message
            if (not has_rag_content and not has_kg_content) and "couldn't find" not in answer.lower() and "no information" not in answer.lower():
                self.logger.warning("synthesis_generated_answer_without_sources", extra={"context": {"query": query[:50], "answer_preview": answer[:100]}})
                return "I couldn't find information about that in the podcast knowledge base. Could you rephrase your question or ask about a specific topic related to philosophy, creativity, coaching, or personal development from the podcasts?"
            
            return answer
        except Exception as e:
            self.logger.error(f"Synthesis failed: {e}")
            return f"Here's what I found:\n\n{rag_context[:500] if rag_context else kg_context[:500]}"
    
    def _synthesize_answer_streaming(
        self,
        query: str,
        resolved_query: str,
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        coverage_info: Optional[Dict[str, Any]] = None,
        mentioned_entities: Optional[List[str]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ):
        """
        Synthesize answer with streaming support - yields chunks as they're generated.
        
        Yields:
            str: Chunks of the answer as they're generated
        """
        import os
        
        # Format RAG context with full metadata
        rag_context = ""
        if rag_results:
            rag_parts = []
            for i, r in enumerate(rag_results, 1):
                text = r.get("text", "")[:400]
                metadata = r.get("metadata", {})
                
                # Extract episode info
                episode_id = metadata.get("episode_id", "")
                source_path = metadata.get("source_path", "")
                if not episode_id or episode_id == "unknown":
                    if source_path:
                        filename = os.path.basename(source_path)
                        episode_id = os.path.splitext(filename)[0]
                
                speaker = metadata.get("speaker", "")
                timestamp = metadata.get("timestamp", "")
                
                # Format source line
                speaker_display = speaker if speaker and speaker != "Unknown" else ""
                episode_display = ""
                
                if episode_id and episode_id != "unknown":
                    parts = episode_id.split("_")
                    if len(parts) >= 2:
                        name_parts = [p for p in parts[1:] if p.upper() not in ["PART", "1", "2", "3"]]
                        if name_parts:
                            episode_display = " ".join(name_parts).title()
                    if not episode_display:
                        episode_display = episode_id.replace("_", " ")
                
                if not speaker_display or speaker_display in ["Speaker 1", "Speaker 2", "Speaker"]:
                    speaker_display = episode_display or "Speaker"
                
                source_info = f"--- {speaker_display}"
                if episode_id and episode_id != "unknown":
                    source_info += f" (Episode: {episode_id})"
                if timestamp:
                    source_info += f" at {timestamp}"
                source_info += " ---"
                
                rag_parts.append(f"{source_info}\n\"{text}\"")
            rag_context = "\n\n".join(rag_parts)
        
        # Format KG context
        kg_context = ""
        if kg_results:
            kg_parts = []
            for r in kg_results:
                concept = r.get("concept", "")
                desc = r.get("description", "")
                rels = r.get("relationships", [])
                
                part = f"**{concept}**"
                if desc:
                    part += f": {desc[:200]}"
                if rels:
                    rel_strs = [f"{rel['rel']} ‚Üí {rel['target']}" for rel in rels[:3]]
                    part += f" (Relationships: {', '.join(rel_strs)})"
                kg_parts.append(part)
            kg_context = "\n".join(kg_parts)
        
        # Conversation context
        conv_context = ""
        if conversation_history:
            recent = conversation_history[-5:]
            conv_parts = [f"{m.get('role', 'user')}: {m.get('content', '')[:200]}" for m in recent]
            conv_context = "\n".join(conv_parts)
        
        # Build entity coverage constraint
        entity_coverage_constraint = ""
        if mentioned_entities and len(mentioned_entities) > 1:
            mentioned_list = ', '.join([e.title() for e in mentioned_entities])
            
            if coverage_info and not coverage_info["all_covered"]:
                missing = ", ".join([e.title() for e in coverage_info["missing"]])
                covered = ", ".join([e.title() for e in coverage_info["covered"]])
            else:
                missing = ""
                covered = mentioned_list
            
            entity_coverage_constraint = f"""
CRITICAL ENTITY COVERAGE REQUIREMENT (MULTI-ENTITY QUERY):
The user specifically asked about these people: {mentioned_list}

STRICT RULES - YOU MUST FOLLOW:
1. ONLY use sources from: {mentioned_list}
2. DO NOT use sources from ANYONE else
3. If you don't have sources for ALL mentioned entities, be explicit
4. DO NOT use proxy speakers or related people as substitutes

ALLOWED SPEAKERS FOR THIS QUERY: {mentioned_list}
FORBIDDEN: Any speaker NOT in the list above"""
        
        # Get style/tone instructions
        style_tone_instructions = self._get_style_tone_instructions(session_metadata)
        
        # Synthesize with LLM - STREAMING (same enhanced prompt as non-streaming)
        system_prompt = f"""You are Sage, a warm and intellectually curious Podcast Intelligence Assistant. You help people explore insights from fascinating podcast conversations.

CRITICAL: You ONLY answer questions about podcast transcripts, knowledge graph content, and topics related to philosophy, creativity, coaching, and personal development. If a question is about math, general knowledge, current events, or anything outside your domain, you MUST politely redirect the user to your domain. Do NOT attempt to answer questions outside your expertise.

MANDATORY SOURCE REQUIREMENT:
- You MUST base your answer ONLY on the TRANSCRIPT SOURCES and KNOWLEDGE GRAPH content provided below
- If the sources don't contain relevant information, you MUST say "I couldn't find information about that in the podcast knowledge base"
- DO NOT use general knowledge or information not in the provided sources
- DO NOT make up information or cite sources that don't exist
{entity_coverage_constraint}

CORE PERSONALITY - BE ENGAGING AND HUMAN:
- You're a thoughtful friend who genuinely loves exploring ideas - show that passion!
- Speak naturally, like you're having a real conversation, not reading a report
- Vary your sentence structure - mix short punchy sentences with longer flowing ones
- Show genuine curiosity and excitement about the insights you're sharing
- Make connections feel organic and interesting, not forced or mechanical
- Use natural transitions: "What's really interesting here is...", "This connects to...", "I love how..."
- If you remember the user's name or previous context, reference it naturally

{style_tone_instructions}

CRITICAL RULES FOR ACCURACY:
1. ONLY cite and reference speakers/episodes that appear in the TRANSCRIPT SOURCES below
2. DO NOT mention or cite anyone not in the provided sources
3. If a speaker is not in the sources, DO NOT reference them - state this explicitly
4. Cite naturally by speaker name - weave it into the narrative (e.g., "Phil Jackson shares this fascinating insight..." or "As Marlon Brando puts it..." or "Rick Rubin describes how...")
5. NEVER use formulaic citations like "[Source 1]" or "[According to Episode X]" - make it feel natural
6. If the sources don't answer the question, explicitly state: "I couldn't find information about that in the podcast knowledge base"

RESPONSE STYLE - MAKE IT ENGAGING:
- Start with something interesting or engaging, not dry facts ("This is fascinating!" or "What's really interesting here is..." or "I love this question because...")
- Tell a story or paint a picture when possible - don't just list facts
- Vary how you present information:
  * Sometimes start with a specific example, then generalize
  * Sometimes start with the big picture, then dive into details
  * Sometimes use a question to engage: "You know what's interesting about this?"
- Weave citations naturally into your narrative - make them feel like part of the story
- Show genuine enthusiasm: "This is such a powerful insight!", "What's amazing is...", "I find this fascinating because..."
- Make connections feel organic: "This connects beautifully to...", "What's interesting is how this relates to...", "You'll notice a pattern here..."
- Use natural language: "tends to", "often", "sometimes", "in many cases" - avoid absolutes unless clearly supported
- End with something engaging: a question, an invitation to explore, or a thoughtful observation

AVOID DRY/MACHINE-LIKE LANGUAGE:
- DON'T start with "According to the sources..." or "The data shows..." (too robotic)
- DON'T use formulaic structures: "First... Second... Third..." (unless listing is truly needed)
- DON'T be overly structured - let it flow naturally
- DON'T use academic language unless style is "academic"
- DON'T list facts without context or story
- DON'T use repetitive sentence structures

MAKE IT FEEL HUMAN:
- Use natural transitions and connectors
- Show personality and genuine interest
- Vary your approach - don't use the same structure every time
- Make it conversational, not report-like
- Let your enthusiasm show through
- Connect ideas in ways that feel organic, not forced

WHAT YOU CANNOT DO:
- Reference speakers not in the provided sources
- Make up quotes or information
- Use "[Source 1]" style citations
- Infer shared principles without direct evidence
- Answer from general knowledge if sources don't contain the answer
- Be dry, robotic, or machine-like

FORMATTING:
- Use natural paragraphs that flow (2-4 sentences typically)
- For lists, use proper markdown with "- " prefix, but make them conversational
- Use **bold** sparingly for emphasis on key concepts
- Keep formatting simple and clean, but let personality shine through"""

        # Build source sections - be explicit about empty sources (same as non-streaming)
        has_rag_content = bool(rag_context and rag_context.strip() and rag_context != "No relevant transcripts found.")
        has_kg_content = bool(kg_context and kg_context.strip() and kg_context != "No relevant concepts found.")
        
        transcript_section = rag_context if has_rag_content else "No relevant transcripts found."
        kg_section = kg_context if has_kg_content else "No relevant concepts found."
        
        # CRITICAL: If both sources are empty, make it VERY clear to LLM
        if not has_rag_content and not has_kg_content:
            transcript_section = "‚ö†Ô∏è NO TRANSCRIPT SOURCES AVAILABLE - DO NOT ANSWER FROM GENERAL KNOWLEDGE"
            kg_section = "‚ö†Ô∏è NO KNOWLEDGE GRAPH SOURCES AVAILABLE - DO NOT ANSWER FROM GENERAL KNOWLEDGE"
        
        user_prompt = f"""Question: {query}

TRANSCRIPT SOURCES (cite by speaker name and episode, NOT by source number):
{transcript_section}

KNOWLEDGE GRAPH:
{kg_section}

{f"CONVERSATION CONTEXT:{chr(10)}{conv_context}" if conv_context else ""}

YOUR TASK:
Create an engaging, natural, and human-sounding response that:
- Feels like a real conversation, not a report or list
- Shows genuine interest and enthusiasm
- Weaves citations naturally into the narrative
- Makes connections feel organic and interesting
- Varies sentence structure and presentation style
- Tells a story or paints a picture when possible

CRITICAL RULES - YOU MUST FOLLOW:
1. Base your answer ONLY on the TRANSCRIPT SOURCES and KNOWLEDGE GRAPH content above
2. If the sources say "No relevant transcripts found" or "No relevant concepts found", you MUST respond with: "I couldn't find information about that in the podcast knowledge base. Could you rephrase your question or ask about a specific topic related to philosophy, creativity, coaching, or personal development from the podcasts?"
3. DO NOT use general knowledge, common sense, or information not in the provided sources
4. DO NOT make up information or cite sources that don't exist
5. If you cannot answer from the sources, explicitly state that you couldn't find information
6. Make citations feel natural - weave speaker names into your narrative, don't use formulaic citations

REMEMBER: Your goal is to make the reader feel engaged and interested, not like they're reading a dry report. Show personality, enthusiasm, and genuine curiosity while maintaining strict accuracy. {f"IMPORTANT: Be honest about missing coverage for any entities." if coverage_info and not coverage_info["all_covered"] else ""}"""

        try:
            # Build proper messages array with conversation history
            messages = [{"role": "system", "content": system_prompt}]
            
            # Add conversation history
            if conversation_history:
                for msg in conversation_history[-5:]:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content:
                        messages.append({"role": role, "content": content})
            
            # Add current query
            messages.append({"role": "user", "content": user_prompt})
            
            # Stream response
            stream = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.5,
                # max_tokens removed to allow full-length responses
                stream=True,  # Enable streaming
            )
            
            # Yield chunks as they arrive
            for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta and delta.content:
                        yield delta.content
                        
        except Exception as e:
            self.logger.error(f"Streaming synthesis failed: {e}")
            yield f"Here's what I found:\n\n{rag_context[:500] if rag_context else kg_context[:500]}"

    def _format_episode_name(self, episode_id: str) -> str:
        """
        Format episode ID to readable name.
        
        Examples:
        - "143_TYLER_COWEN_PART_1" ‚Üí "Tyler Cowen (Episode 143)"
        - "001_PHIL_JACKSON" ‚Üí "Phil Jackson (Episode 001)"
        - "002_JERROD_CARMICHAEL" ‚Üí "Jerrod Carmichael (Episode 002)"
        """
        if not episode_id or episode_id == "unknown":
            return "Unknown Episode"
        
        parts = episode_id.split("_")
        if len(parts) >= 2:
            # Extract episode number (first part)
            episode_number = parts[0]
            
            # Extract name parts (skip "PART", "1", "2", "3", etc.)
            name_parts = [
                p for p in parts[1:] 
                if p.upper() not in ["PART", "1", "2", "3", "4", "5", "ONE", "TWO", "THREE"]
            ]
            
            if name_parts:
                # Join and title case: "TYLER_COWEN" ‚Üí "Tyler Cowen"
                name = " ".join(name_parts).title()
                return f"{name} (Episode {episode_number})"
        
        # Fallback: replace underscores with spaces
        return episode_id.replace("_", " ").title()
    
    def _format_timestamp(self, timestamp: str) -> str:
        """
        Format timestamp to readable format.
        
        Examples:
        - "00:15:30" ‚Üí "15:30"
        - "01:30:45" ‚Üí "1:30:45"
        - "00:05:10" ‚Üí "5:10"
        """
        if not timestamp:
            return ""
        
        # Handle different timestamp formats
        timestamp = timestamp.strip()
        
        # Format: HH:MM:SS
        if ":" in timestamp:
            parts = timestamp.split(":")
            if len(parts) == 3:
                hours, minutes, seconds = parts
                try:
                    hours_int = int(hours)
                    if hours_int == 0:
                        # Less than 1 hour: show MM:SS
                        return f"{int(minutes)}:{seconds.zfill(2)}"
                    else:
                        # 1+ hours: show H:MM:SS
                        return f"{hours_int}:{minutes.zfill(2)}:{seconds.zfill(2)}"
                except ValueError:
                    return timestamp
            elif len(parts) == 2:
                # Format: MM:SS
                return timestamp
        
        return timestamp
    
    def _resolve_speaker(
        self,
        metadata: Dict[str, Any],
        episode_id: str,
    ) -> str:
        """
        Resolve speaker name from metadata or episode.
        
        Priority:
        1. metadata.speaker (if not generic)
        2. Episode name (extracted from episode_id)
        3. "Unknown Speaker"
        """
        # Try metadata speaker first
        speaker = (
            metadata.get("speaker") or 
            metadata.get("speaker_name") or
            metadata.get("author") or
            ""
        )
        
        # Check if speaker is generic/unknown
        generic_speakers = [
            "unknown", "speaker 1", "speaker 2", "speaker", 
            "speaker1", "speaker2", "host", "guest", ""
        ]
        
        if speaker and speaker.lower() not in generic_speakers:
            return speaker
        
        # Fallback: Extract from episode name
        if episode_id and episode_id != "unknown":
            formatted_episode = self._format_episode_name(episode_id)
            # Extract name part: "Tyler Cowen (Episode 143)" ‚Üí "Tyler Cowen"
            if " (Episode " in formatted_episode:
                return formatted_episode.split(" (Episode ")[0]
            return formatted_episode
        
        return "Unknown Speaker"
    
    def _calculate_confidence(
        self,
        result: Dict[str, Any],
        all_results: List[Dict[str, Any]],
        source_type: str = "rag",
    ) -> float:
        """
        Calculate source confidence score.
        
        Factors:
        - Relevance score (from RAG/KG)
        - Number of sources mentioning similar content
        - Source type (KG = more reliable)
        
        Returns:
            Confidence score (0.0 - 1.0)
        """
        # Start with base score from result
        score = result.get("score", 0.5)
        
        # Normalize score to 0-1 range if needed
        if score > 1.0:
            score = score / 100.0  # Assume 0-100 scale
        elif score < 0.0:
            score = 0.5  # Default if negative
        
        # Boost if mentioned in multiple sources (corroboration)
        text = result.get("text", "")[:100]  # First 100 chars for comparison
        if text:
            similar_count = sum(
                1 for r in all_results 
                if text.lower() in r.get("text", "").lower()[:100]
            )
            if similar_count > 1:
                # Boost confidence if multiple sources mention similar content
                score += min(0.15, (similar_count - 1) * 0.05)
        
        # Boost if from KG (structured knowledge is more reliable)
        if source_type == "kg":
            score += 0.1
        
        # Cap at 1.0
        return min(score, 1.0)
    
    def _extract_sources(
        self,
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """
        Extract sources with ENHANCED metadata formatting.
        
        Improvements:
        - Formatted episode names (e.g., "Tyler Cowen (Episode 143)")
        - Formatted timestamps (e.g., "15:30")
        - Resolved speaker names
        - Confidence scores
        """
        import os
        sources = []
        seen = set()
        
        # Combine all results for confidence calculation
        all_results = rag_results + kg_results
        
        # Extract RAG sources with enhanced formatting
        for r in rag_results:
            metadata = r.get("metadata", {})
            
            # Try multiple possible field names for episode_id
            episode_id = (
                metadata.get("episode_id") or 
                metadata.get("episode") or 
                metadata.get("file_name") or
                metadata.get("filename") or
                ""
            )
            
            source_path = metadata.get("source_path") or metadata.get("source") or metadata.get("path") or ""
            
            # Extract episode from path if needed
            if not episode_id or episode_id == "unknown":
                if source_path:
                    filename = os.path.basename(source_path)
                    episode_id = os.path.splitext(filename)[0]
            
            # Format episode name
            episode_name = self._format_episode_name(episode_id)
            
            # Resolve speaker
            speaker = self._resolve_speaker(metadata, episode_id)
            
            # Format timestamp
            timestamp_raw = (
                metadata.get("timestamp") or 
                metadata.get("start_time") or
                metadata.get("time") or
                ""
            )
            timestamp = self._format_timestamp(timestamp_raw)
            
            text = r.get("text", "")[:200]
            
            # Calculate confidence
            confidence = self._calculate_confidence(r, all_results, source_type="rag")
            
            # Deduplicate by episode + speaker + timestamp
            key = f"{episode_id}:{speaker}:{timestamp}"
            if key not in seen:
                seen.add(key)
                sources.append({
                    "type": "transcript",
                    "episode_id": episode_id or "Unknown Episode",
                    "episode_name": episode_name,  # Formatted name
                    "speaker": speaker,  # Resolved speaker
                    "timestamp": timestamp,  # Formatted timestamp
                    "timestamp_raw": timestamp_raw,  # Keep raw for reference
                    "text": text,
                    "source_path": source_path,
                    "confidence": round(confidence, 2),  # Confidence score
                    "score": r.get("score", 0.5),  # Original relevance score
                })
        
        # Extract KG sources with enhanced formatting
        for r in kg_results:
            concept = r.get("concept", "") or r.get("name", "")
            if concept:
                # Extract episode IDs from KG result if available
                episode_ids = r.get("episode_ids", [])
                episode_names = []
                
                if episode_ids:
                    for ep_id in episode_ids[:3]:  # Limit to first 3 episodes
                        formatted = self._format_episode_name(str(ep_id))
                        episode_names.append(formatted)
                
                # Calculate confidence
                confidence = self._calculate_confidence(r, all_results, source_type="kg")
                
                sources.append({
                    "type": "knowledge_graph",
                    "concept": concept,
                    "node_type": r.get("type", "Concept"),
                    "description": r.get("description", "")[:100],
                    "episode_ids": episode_ids[:3] if episode_ids else [],
                    "episode_names": episode_names,  # Formatted episode names
                    "confidence": round(confidence, 2),
                    "score": r.get("relevance", 0.5) if isinstance(r.get("relevance"), (int, float)) else 0.5,
                })
        
        # Sort by confidence (highest first)
        sources.sort(key=lambda x: x.get("confidence", 0.0), reverse=True)
        
        return sources

    def _update_active_entity(
        self,
        query: str,
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]],
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Update active entity for follow-up questions."""
        if not session_metadata:
            return
        
        # Extract entity from query
        patterns = [
            r"who is ([a-zA-Z\s]+)\??",
            r"what does ([a-zA-Z\s]+) (?:say|think|believe)",
            r"tell me about ([a-zA-Z\s]+)",
            r"([A-Z][a-z]+ [A-Z][a-z]+)",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                session_metadata["active_entity"] = match.group(1).strip().title()
                return
        
        # Or from KG results
        for r in kg_results:
            if r.get("type") == "Person":
                session_metadata["active_entity"] = r.get("concept")
                return
