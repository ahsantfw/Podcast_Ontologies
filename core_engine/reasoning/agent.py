"""
PODCAST INTELLIGENCE AGENT v3 - FULLY LLM-DRIVEN

A truly intelligent agent that:
1. Uses LLM for ALL responses (no hard-coded text)
2. Dynamically queries Neo4j based on user intent
3. Properly extracts and displays sources with all metadata
4. Handles complex queries intelligently

ARCHITECTURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LLM BRAIN (Decides Everything)                       â”‚
â”‚  - Understands user intent semantically                                  â”‚
â”‚  - Generates Cypher queries dynamically                                  â”‚
â”‚  - Synthesizes natural responses                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                       â–¼                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RAG Tool    â”‚      â”‚   KG Tool     â”‚      â”‚   Memory      â”‚
    â”‚   (Qdrant)    â”‚      â”‚   (Neo4j)     â”‚      â”‚   (Context)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

from __future__ import annotations

from typing import Optional, Dict, Any, List
from dataclasses import dataclass, field
import json
import os
import re
from dotenv import load_dotenv

from core_engine.logging import get_logger

load_dotenv()


@dataclass
class AgentResponse:
    """Response from the agent."""
    answer: str
    tools_used: List[str] = field(default_factory=list)
    sources: List[Dict[str, Any]] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


class PodcastAgent:
    """
    Fully LLM-Driven Podcast Intelligence Agent.
    
    NO hard-coded responses - everything is generated by LLM.
    """
    
    # SYSTEM PURPOSE - Core mission and capabilities
    SYSTEM_PURPOSE = """
    PODCAST INTELLIGENCE ASSISTANT - System Purpose & Capabilities
    
    MISSION:
    Help users explore insights from podcast transcripts about philosophy, creativity, 
    coaching, and personal development through intelligent querying of a knowledge graph.
    
    DOMAIN:
    - ðŸ§  Philosophy: Deep thinking, wisdom, mental models, meaning of life
    - ðŸŽ¨ Creativity: Innovation, artistic process, creative practices, inspiration
    - ðŸ† Coaching: Leadership, performance, team dynamics, personal growth
    - ðŸ§˜ Personal Development: Mindfulness, meditation, self-awareness, well-being
    
    CAPABILITIES:
    âœ… Answer questions about concepts, people, and ideas from podcasts
    âœ… Find relationships between different concepts (Practice â†’ Outcome)
    âœ… Retrieve relevant quotes with full source attribution (episode, speaker, timestamp)
    âœ… Discover patterns across multiple episodes (cross-episode analysis)
    âœ… Multi-hop reasoning (X â†’ Y â†’ Z connections)
    âœ… Speaker-anchored queries ("What did Phil Jackson say about...?")
    
    WHAT I KNOW:
    - Structured knowledge graph (Neo4j) with concepts, practices, outcomes, relationships
    - Vector search (Qdrant) for semantic transcript search
    - Full conversation history and context
    - Episode metadata (episode_id, speaker, timestamp)
    
    WHAT I DON'T KNOW:
    âŒ Current events, news, real-time information
    âŒ General world knowledge outside podcast corpus
    âŒ Weather, sports scores, stock prices
    âŒ How to write code or debug programs
    âŒ Recipes, geography facts, general trivia
    
    QUERY TYPES I HANDLE:
    1. KNOWLEDGE_QUERY: Questions about podcast content
       - "What is creativity?"
       - "Who is Phil Jackson?"
       - "What practices help with anxiety?"
       - "What did Rick Rubin say about the creative process?"
    
    2. RELATIONSHIP_QUERY: How concepts connect
       - "How does meditation relate to focus?"
       - "What practices optimize creativity?"
       - "What leads to better decision-making?"
    
    3. PATTERN_QUERY: Cross-episode patterns
       - "What concepts appear across multiple episodes?"
       - "What are shared principles about creativity?"
       - "What do successful people have in common?"
    
    4. SOURCE_QUERY: Finding specific quotes or sources
       - "Find quotes about meditation"
       - "What did speaker X say about topic Y?"
       - "Show me sources about creativity"
    
    INTELLIGENT QUERY PROCESSING:
    - Understand user intent in context of system purpose
    - Route to appropriate tool (RAG, KG, or direct response)
    - Synthesize multi-source answers with full attribution
    - Maintain conversation context for follow-up questions
    """
    
    # Out of scope patterns - these get refused WITHOUT searching RAG/KG
    OUT_OF_SCOPE_PATTERNS = [
        r"\b(weather|temperature|forecast)\b",
        r"\b(president|prime minister|politician|election|vote|government)\b",
        r"\b(stock|crypto|bitcoin|price|market|trading)\b",
        r"\b(sports? score|game score|who won the game|match result)\b",
        r"\b(write code|program|python|javascript|sql query|debug)\b",
        r"\b(recipe|cook|food|restaurant|menu)\b",
        r"\b(news today|current events|breaking news)\b",
        r"\b(capital of|population of|geography)\b",
        r"\bwho is (?:the )?(?:pm|president|king|queen|leader) of\b",
        r"\b(translate|translation|language)\b",
        r"\b(calculate|math|equation|solve)\b",
    ]

    def __init__(
        self,
        workspace_id: str = "default",
        model: str = "gpt-4o-mini",
        hybrid_retriever=None,
        neo4j_client=None,
    ):
        self.workspace_id = workspace_id
        self.model = model
        self.hybrid_retriever = hybrid_retriever
        self.neo4j_client = neo4j_client
        self.logger = get_logger("core_engine.reasoning.agent", workspace_id=workspace_id)
        
        # Initialize OpenAI
        try:
            from openai import OpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found")
            self.openai_client = OpenAI(api_key=api_key)
        except Exception as e:
            self.logger.error(f"OpenAI init failed: {e}")
            self.openai_client = None
        
        self.logger.info("agent_v3_initialized", extra={
            "workspace_id": workspace_id,
            "has_rag": hybrid_retriever is not None,
            "has_kg": neo4j_client is not None,
        })

    def run(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """
        Run the agent - LLM decides everything.
        """
        if not self.openai_client:
            return AgentResponse(
                answer="I'm having trouble connecting. Please try again.",
                metadata={"error": "OpenAI client not available"}
            )
        
        # Ensure session_metadata exists
        if session_metadata is None:
            session_metadata = {}
        
        # Log session state for debugging
        self.logger.info(f"Session metadata at start: user_name={session_metadata.get('user_name')}, active_entity={session_metadata.get('active_entity')}")
        
        query_lower = query.lower().strip()
        
        # Check for out of scope FIRST (before any LLM calls)
        if self._is_out_of_scope(query_lower):
            self.logger.info(f"Query detected as OUT OF SCOPE: {query[:50]}")
            return self._handle_out_of_scope_llm(query, conversation_history)
        
        # Let LLM decide what to do
        intent = self._classify_intent_llm(query, conversation_history, session_metadata)
        
        self.logger.info(f"LLM classified intent: {intent}", extra={"query": query[:50]})
        
        # Route based on LLM's decision
        if intent == "greeting":
            # Check if user is introducing themselves
            self._extract_user_info(query, session_metadata)
            return self._handle_with_llm(query, conversation_history, "greeting")
        
        if intent == "conversational":
            # Check if user is sharing info about themselves
            self._extract_user_info(query, session_metadata)
            return self._handle_with_llm(query, conversation_history, "conversational")
        
        if intent == "system_info":
            return self._handle_with_llm(query, conversation_history, "system_info")
        
        if intent == "user_memory":
            return self._handle_user_memory(query, conversation_history, session_metadata)
        
        if intent == "list_kg":
            return self._handle_list_kg_dynamic(query, conversation_history)
        
        if intent == "kg_query":
            return self._handle_kg_query_dynamic(query, conversation_history, session_metadata)
        
        # Default: knowledge query with RAG + KG
        return self._handle_knowledge_query(query, conversation_history, session_metadata)

    def _is_out_of_scope(self, query_lower: str) -> bool:
        """Check if query is clearly out of scope."""
        for pattern in self.OUT_OF_SCOPE_PATTERNS:
            if re.search(pattern, query_lower, re.IGNORECASE):
                return True
        return False

    def _classify_intent_llm(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Use LLM to classify intent."""
        
        # Build context
        context = ""
        if conversation_history:
            recent = conversation_history[-5:]
            context = "\n".join([f"{m.get('role', 'user')}: {m.get('content', '')[:100]}" for m in recent])
        
        # Check for user memory in session
        user_info = ""
        if session_metadata:
            user_name = session_metadata.get("user_name", "")
            user_facts = session_metadata.get("user_facts", [])
            if user_name or user_facts:
                user_info = f"\nUSER INFO STORED: name={user_name}, facts={user_facts}"
        
        prompt = f"""You are an intelligent intent classifier for a Podcast Intelligence System.

{self.SYSTEM_PURPOSE}

YOUR TASK: Classify the user query into ONE category based on:
1. The system's purpose and capabilities
2. Whether the query aligns with the domain (philosophy, creativity, coaching, personal development)
3. Whether the query requires searching the knowledge base

CATEGORIES:
- greeting: Simple hello/hi/hey
- conversational: Casual chat, thanks, how are you, reactions (hmm, ok, wow)
- system_info: Questions about what this system is/does (capabilities, purpose)
- user_memory: Questions about what the user told you (their name, preferences, things they said)
- list_kg: Requests to list/show ALL concepts, nodes, or relationship types from knowledge graph
- kg_query: Questions about SPECIFIC relationships (e.g., "what is ABOUT", "show ENABLES", "what does X relate to")
- knowledge_query: Questions seeking information from podcast content (concepts, people, practices, quotes)

INTELLIGENT CLASSIFICATION RULES:
1. If query asks "how many transcripts/episodes/concepts" â†’ knowledge_query (needs to query data)
2. If query is about podcast content (people, concepts, practices, ideas, "what practices are associated with X") â†’ knowledge_query
3. If query asks about system structure ("list concepts", "show relationships") â†’ list_kg
4. If query asks about specific relationship types ("what is ABOUT?", "show ENABLES") â†’ kg_query
5. If query is clearly out of scope (weather, news, code) â†’ Already filtered, won't reach here
6. Consider the system's domain - if query doesn't relate to philosophy/creativity/coaching/personal development, 
   but is general knowledge, be honest about limitations

EXAMPLES:
- "list all concepts" â†’ list_kg
- "what are all relationships" â†’ list_kg  
- "what is the ABOUT relationship" â†’ kg_query (asking about relationship TYPE)
- "show me ENABLES connections" â†’ kg_query (asking about relationship TYPE)
- "what practices are most associated with clarity" â†’ knowledge_query (asking about CONTENT)
- "what concepts appear across multiple episodes" â†’ knowledge_query (asking about CONTENT)
- "what did Phil Jackson say about creativity" â†’ knowledge_query (asking about CONTENT)
- "what is creativity" â†’ knowledge_query (asking about concept from podcasts)
- "how many transcripts/episodes/concepts" â†’ knowledge_query (asking about data/counts)
- "who is the PM of Pakistan" â†’ Already filtered (out of scope)

CRITICAL DISTINCTION:
- kg_query: Questions about the STRUCTURE of the knowledge graph (relationship types, graph structure)
- knowledge_query: Questions about the CONTENT in the knowledge graph (practices, concepts, people, quotes, counts)

CONVERSATION CONTEXT:
{context if context else "No prior context"}
{user_info}

USER QUERY: "{query}"

Think intelligently: Does this query align with the system's purpose? What is the user really asking for?
Respond with ONLY the category name (one word):"""

        try:
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an intelligent intent classifier for a Podcast Intelligence System. You understand the system's purpose and capabilities. Classify queries by considering what the user is really asking and whether it aligns with the system's domain. Respond with only the category name."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=20,
            )
            intent = response.choices[0].message.content.strip().lower()
            
            # Validate intent
            valid_intents = {"greeting", "conversational", "system_info", "user_memory", "list_kg", "kg_query", "knowledge_query"}
            if intent not in valid_intents:
                return "knowledge_query"
            return intent
        except Exception as e:
            self.logger.error(f"Intent classification failed: {e}")
            return "knowledge_query"

    def _handle_with_llm(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        intent_type: str = "general",
    ) -> AgentResponse:
        """Handle any intent with LLM - no hard-coded responses."""
        
        # Build context
        context = ""
        if conversation_history:
            recent = conversation_history[-5:]
            context = "\n".join([f"{m.get('role', 'user')}: {m.get('content', '')[:150]}" for m in recent])
        
        # Intent-specific instructions with engaging personality
        instructions = {
            "greeting": """You are an enthusiastic Podcast Intelligence Assistant named Sage - a curious explorer of ideas from fascinating podcast conversations.

PERSONALITY:
- Warm, intellectually curious, and genuinely excited about helping
- You love connecting dots between ideas from different thinkers
- You speak like a thoughtful friend who's passionate about learning
- Remember names if shared, and reference past conversations naturally

RESPONSE STYLE:
- Greet warmly and show genuine interest
- Share something intriguing about what you can help with (philosophy, creativity, coaching, personal development)
- Ask an engaging question to invite exploration
- Keep it natural (2-3 sentences)
- You can use 1 emoji to add warmth

Example tone: "Hey! Great to see you. I've been diving into some fascinating conversations about creativity and mindfulness. What's on your mind today?"
""",
            
            "conversational": """You are Sage, a warm and intellectually curious Podcast Intelligence Assistant.

PERSONALITY:
- Genuinely interested in the person you're talking to
- Responds with empathy and warmth
- Makes connections to relevant podcast insights when natural
- Remembers what users have shared and references it
- Speaks like a thoughtful friend, not a formal assistant

RESPONSE STYLE:
- Be natural and conversational
- Show you're listening and engaged
- If they share something about themselves, acknowledge it genuinely
- Gently invite deeper exploration when appropriate
- Keep responses brief but warm (1-2 sentences)
""",
            
            "system_info": f"""You are Sage, an enthusiastic Podcast Intelligence Assistant who loves exploring ideas.

{self.SYSTEM_PURPOSE}

PERSONALITY when explaining yourself:
- Be genuinely excited about your capabilities
- Speak like you're sharing something cool with a friend
- Give specific, intriguing examples of questions
- Show your personality - you're curious, thoughtful, and helpful

When explaining:
1. Share your MISSION with enthusiasm
2. Explain what makes you unique (connecting ideas across conversations)
3. Give 2-3 specific, intriguing example questions that spark curiosity
4. Invite them to explore with you

Example tone: "I'm like your personal guide through some of the most fascinating conversations about creativity, philosophy, and human potential. I can help you discover what Phil Jackson thinks about mindfulness in leadership, or how Rick Rubin approaches creativity..."

Use plain text, avoid complex markdown. Be concise but engaging.""",
        }
        
        instruction = instructions.get(intent_type, instructions["conversational"])
        
        prompt = f"""{instruction}

{f"CONVERSATION CONTEXT:{chr(10)}{context}" if context else ""}

USER: "{query}"

Respond naturally:"""

        try:
            # Build messages array with conversation history for proper context
            messages = [{"role": "system", "content": instruction}]
            
            # Add conversation history as proper messages (not just text context)
            if conversation_history:
                for msg in conversation_history[-5:]:  # Last 5 messages
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content:
                        messages.append({"role": role, "content": content})
            
            # Add current query
            messages.append({"role": "user", "content": query})
            
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=300,
            )
            return AgentResponse(
                answer=response.choices[0].message.content,
                metadata={"type": intent_type}
            )
        except Exception as e:
            self.logger.error(f"LLM response failed: {e}")
            return AgentResponse(
                answer="I'm here to help! What would you like to know about the podcasts?",
                metadata={"type": intent_type, "error": str(e)}
            )

    def _extract_user_info(
        self,
        query: str,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Extract and store user information from their messages."""
        if session_metadata is None:
            self.logger.warning("Cannot extract user info - session_metadata is None")
            return
        
        query_lower = query.lower()
        
        # Extract name patterns
        name_patterns = [
            r"(?:my name is|i am|i'm|call me|this is)\s+([a-zA-Z]+)",
            r"^([a-zA-Z]+)\s+here$",
        ]
        
        for pattern in name_patterns:
            match = re.search(pattern, query_lower, re.IGNORECASE)
            if match:
                name = match.group(1).strip().title()
                if len(name) > 1 and name.lower() not in {"hi", "hello", "hey", "ok", "yes", "no", "feeling", "not", "great", "good", "bad"}:
                    session_metadata["user_name"] = name
                    self.logger.info(f"STORED USER NAME: {name} in session_metadata")
                    return
        
        # Store other user facts
        fact_patterns = [
            r"i (?:am|work as|like|love|enjoy|prefer)\s+(.+)",
            r"i'm (?:a|an|from|interested in)\s+(.+)",
        ]
        
        for pattern in fact_patterns:
            match = re.search(pattern, query_lower, re.IGNORECASE)
            if match:
                fact = match.group(0).strip()
                if "user_facts" not in session_metadata:
                    session_metadata["user_facts"] = []
                if fact not in session_metadata["user_facts"]:
                    session_metadata["user_facts"].append(fact)
                    self.logger.info(f"STORED USER FACT: {fact}")
                return

    def _handle_user_memory(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """Handle questions about what the user told us."""
        
        # Get stored user info
        user_name = session_metadata.get("user_name", "") if session_metadata else ""
        user_facts = session_metadata.get("user_facts", []) if session_metadata else []
        
        # Build messages array with conversation history
        messages = [
            {"role": "system", "content": f"""You are Sage, a warm and attentive Podcast Intelligence Assistant who remembers what users share.

USER'S STORED INFO:
- Name: {user_name if user_name else "They haven't shared their name yet"}
- Facts they've shared: {user_facts if user_facts else "Nothing stored yet"}

PERSONALITY:
- Warm and genuinely interested in them
- If you remember something, share it naturally like a friend would
- If you don't have the information, be honest but warm - invite them to share
- Never make up information
- Reference the conversation naturally

Example responses:
- If you know their name: "Of course I remember you, [Name]! You mentioned..."
- If you don't know: "I don't think you've mentioned that yet - I'd love to know!"
"""}
        ]
        
        # Add conversation history for context
        if conversation_history:
            for msg in conversation_history[-8:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role in ["user", "assistant"] and content:
                    messages.append({"role": role, "content": content})
        
        messages.append({"role": "user", "content": query})

        try:
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=200,
            )
            return AgentResponse(
                answer=response.choices[0].message.content,
                metadata={"type": "user_memory", "has_name": bool(user_name)}
            )
        except Exception as e:
            self.logger.error(f"User memory response failed: {e}")
            if user_name:
                return AgentResponse(
                    answer=f"Of course! Your name is {user_name}. What else would you like to know?",
                    metadata={"type": "user_memory"}
                )
            return AgentResponse(
                answer="I don't think you've shared that with me yet - I'd love to know!",
                metadata={"type": "user_memory"}
            )

    def _handle_out_of_scope_llm(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
    ) -> AgentResponse:
        """Handle out of scope with LLM - polite refusal."""
        try:
            # Build messages array with conversation history
            messages = [
                {"role": "system", "content": """You are a Podcast Intelligence Assistant with a warm, engaging personality.
The user asked something outside your expertise (you only know about podcasts on philosophy, creativity, coaching, personal development).
Politely explain this is outside your expertise and invite them to ask about podcast topics instead.
Be brief, friendly, and maintain any relationship you've built in the conversation."""}
            ]
            
            # Add conversation history for context
            if conversation_history:
                for msg in conversation_history[-3:]:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content:
                        messages.append({"role": role, "content": content})
            
            messages.append({"role": "user", "content": query})
            
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=150,
            )
            return AgentResponse(
                answer=response.choices[0].message.content,
                metadata={"type": "out_of_scope"}
            )
        except:
            return AgentResponse(
                answer="That's outside my expertise. I specialize in insights from podcasts about philosophy, creativity, coaching, and personal development. What would you like to explore in those areas?",
                metadata={"type": "out_of_scope"}
            )

    def _handle_list_kg_dynamic(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
    ) -> AgentResponse:
        """Handle list/explore KG requests dynamically using LLM to generate Cypher."""
        if not self.neo4j_client:
            return AgentResponse(
                answer="Knowledge graph is not available.",
                metadata={"type": "list_kg", "error": "no_kg"}
            )
        
        # Use LLM to understand what user wants to list
        try:
            # First, understand the request
            understand_prompt = f"""The user wants to list/explore the knowledge graph.
            
User query: "{query}"

What do they want to see? Extract:
1. node_type: Which type of nodes? (Concept, Practice, Person, Principle, Outcome, CognitiveState, BehavioralPattern, or "all")
2. limit: How many? (default 20, max 100)
3. filter: Any specific filter? (e.g., names containing something)

Respond with JSON:
{{"node_type": "...", "limit": 20, "filter": null}}"""

            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Extract structured info from the query. Respond with JSON only."},
                    {"role": "user", "content": understand_prompt}
                ],
                temperature=0.1,
                max_tokens=100,
            )
            
            result_text = response.choices[0].message.content.strip()
            if result_text.startswith("```"):
                result_text = result_text.split("```")[1].replace("json", "").strip()
            
            params = json.loads(result_text)
            node_type = params.get("node_type", "all")
            limit = min(params.get("limit", 20), 100)
            
        except Exception as e:
            self.logger.warning(f"Failed to parse list request: {e}")
            node_type = "all"
            limit = 20
        
        # Build and execute Cypher query
        try:
            if node_type.lower() == "all":
                # Get all types with counts
                cypher = """
                MATCH (c)
                WHERE c.workspace_id = $workspace_id
                  AND (c:Concept OR c:Practice OR c:Person OR c:Principle OR c:Outcome OR c:CognitiveState OR c:BehavioralPattern)
                WITH labels(c)[0] as type, collect(c.name)[..$limit] as names, count(*) as total
                RETURN type, names, total
                ORDER BY total DESC
                """
                results = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id, "limit": limit})
                
                # Get relationship types
                rel_cypher = """
                MATCH (a)-[r]->(b)
                WHERE a.workspace_id = $workspace_id
                RETURN DISTINCT type(r) as relationship, count(*) as count
                ORDER BY count DESC
                LIMIT 15
                """
                rel_results = self.neo4j_client.execute_read(rel_cypher, {"workspace_id": self.workspace_id})
                
                # Format response with proper markdown
                answer_parts = ["## ðŸ“š Knowledge Graph Contents"]
                answer_parts.append("")  # Empty line for markdown
                total_concepts = 0
                
                if results:
                    answer_parts.append("### Concepts by Category")
                    answer_parts.append("")
                    for row in results:
                        type_name = row.get("type", "Other")
                        names = row.get("names", [])
                        total = row.get("total", 0)
                        total_concepts += total
                        
                        answer_parts.append(f"**{type_name}** ({total} total):")
                        answer_parts.append("")
                        for name in names[:limit]:
                            answer_parts.append(f"- {name}")
                        if total > limit:
                            answer_parts.append(f"- ... and {total - limit} more")
                        answer_parts.append("")
                
                if rel_results:
                    answer_parts.append("### Relationship Types")
                    answer_parts.append("")
                    for row in rel_results:
                        rel = row.get("relationship", "Unknown")
                        count = row.get("count", 0)
                        answer_parts.append(f"- **{rel}** ({count} connections)")
                    answer_parts.append("")
                
                answer_parts.append("---")
                answer_parts.append(f"*Total: {total_concepts} concepts. Ask me about any of these!*")
                
                return AgentResponse(
                    answer="\n".join(answer_parts),
                    tools_used=["neo4j_query"],
                    metadata={"type": "list_kg", "total_concepts": total_concepts}
                )
            else:
                # Get specific type
                cypher = f"""
                MATCH (c:{node_type})
                WHERE c.workspace_id = $workspace_id
                RETURN c.name as name, c.description as description
                ORDER BY c.name
                LIMIT $limit
                """
                results = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id, "limit": limit})
                
                if not results:
                    return AgentResponse(
                        answer=f"No {node_type} nodes found in the knowledge graph.",
                        metadata={"type": "list_kg"}
                    )
                
                # Format response with proper markdown
                answer_parts = [f"## ðŸ“š {node_type} Nodes ({len(results)} shown)"]
                answer_parts.append("")
                for r in results:
                    name = r.get("name", "Unknown")
                    desc = r.get("description", "")
                    if desc:
                        answer_parts.append(f"- **{name}**: {desc[:100]}...")
                    else:
                        answer_parts.append(f"- **{name}**")
                
                return AgentResponse(
                    answer="\n".join(answer_parts),
                    tools_used=["neo4j_query"],
                    metadata={"type": "list_kg", "node_type": node_type, "count": len(results)}
                )
                
        except Exception as e:
            self.logger.error(f"KG list query failed: {e}")
            return AgentResponse(
                answer=f"I couldn't query the knowledge graph: {str(e)}",
                metadata={"type": "list_kg", "error": str(e)}
            )

    def _handle_kg_query_dynamic(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """Handle specific KG queries using LLM-generated Cypher."""
        if not self.neo4j_client:
            return AgentResponse(
                answer="Knowledge graph is not available.",
                metadata={"type": "kg_query", "error": "no_kg"}
            )
        
        query_lower = query.lower()
        
        # Check if user is asking about a specific relationship type
        relationship_types = ["about", "relates_to", "influences", "enables", "leads_to", 
                            "causes", "optimizes", "reduces", "is_part_of", "requires", "enhances"]
        
        for rel_type in relationship_types:
            if rel_type in query_lower:
                # User is asking about a specific relationship - use predefined query
                rel_upper = rel_type.upper()
                cypher = f"""
                MATCH (a)-[r:{rel_upper}]->(b)
                WHERE a.workspace_id = $workspace_id
                RETURN a.name as source, type(r) as relationship, b.name as target, 
                       a.description as source_desc, b.description as target_desc
                LIMIT 20
                """
                self.logger.info(f"Using predefined relationship query for: {rel_upper}")
                
                try:
                    results = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id})
                    
                    if not results:
                        return AgentResponse(
                            answer=f"I couldn't find any {rel_upper} relationships in the knowledge graph.",
                            tools_used=["neo4j_query"],
                            metadata={"type": "kg_query", "relationship": rel_upper}
                        )
                    
                    # Format results
                    answer_parts = [f"## ðŸ”— {rel_upper} Relationships\n"]
                    answer_parts.append(f"Found {len(results)} examples of {rel_upper} relationships:\n")
                    
                    for r in results:
                        source = r.get("source", "Unknown")
                        target = r.get("target", "Unknown")
                        answer_parts.append(f"- **{source}** â†’ {rel_upper} â†’ **{target}**")
                    
                    return AgentResponse(
                        answer="\n".join(answer_parts),
                        tools_used=["neo4j_query"],
                        metadata={"type": "kg_query", "relationship": rel_upper, "count": len(results)}
                    )
                except Exception as e:
                    self.logger.error(f"Relationship query failed: {e}")
                    return AgentResponse(
                        answer=f"I couldn't query the {rel_upper} relationships: {str(e)}",
                        metadata={"type": "kg_query", "error": str(e)}
                    )
        
        # Use LLM to generate Cypher for other queries
        try:
            cypher_prompt = f"""Generate a Cypher query for Neo4j based on this user question.

SCHEMA:
- Node types: Concept, Practice, Person, Principle, Outcome, CognitiveState, BehavioralPattern
- All nodes have: name, description, workspace_id
- Relationship types: RELATES_TO, INFLUENCES, ENABLES, LEADS_TO, CAUSES, OPTIMIZES, REDUCES, IS_PART_OF, ABOUT

RULES:
- Always filter by workspace_id = $workspace_id
- Limit results to 20 unless user specifies
- Return useful fields (name, description, relationships)
- For relationship queries, use: MATCH (a)-[r:REL_TYPE]->(b) WHERE a.workspace_id = $workspace_id RETURN a.name, type(r), b.name

User question: "{query}"

Respond with ONLY the Cypher query, no explanation:"""

            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a Cypher query generator. Output only valid Cypher."},
                    {"role": "user", "content": cypher_prompt}
                ],
                temperature=0.1,
                max_tokens=300,
            )
            
            cypher = response.choices[0].message.content.strip()
            if cypher.startswith("```"):
                cypher = cypher.split("```")[1].replace("cypher", "").strip()
            
            self.logger.info(f"Generated Cypher: {cypher}")
            
            # Execute query
            results = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id})
            
            if not results:
                return AgentResponse(
                    answer="I couldn't find any matching results in the knowledge graph.",
                    tools_used=["neo4j_query"],
                    metadata={"type": "kg_query", "cypher": cypher}
                )
            
            # Use LLM to format results
            format_prompt = f"""Format these Neo4j query results into a natural response.

User question: "{query}"
Results: {json.dumps(results[:10], default=str)}

Provide a clear, well-formatted answer:"""

            format_response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Format database results into a natural, helpful response."},
                    {"role": "user", "content": format_prompt}
                ],
                temperature=0.5,
                max_tokens=500,
            )
            
            return AgentResponse(
                answer=format_response.choices[0].message.content,
                tools_used=["neo4j_query"],
                sources=[{"type": "knowledge_graph", "concept": r.get("name", "")} for r in results[:5] if r.get("name")],
                metadata={"type": "kg_query", "result_count": len(results)}
            )
            
        except Exception as e:
            self.logger.error(f"KG query failed: {e}")
            return AgentResponse(
                answer=f"I couldn't query the knowledge graph: {str(e)}",
                metadata={"type": "kg_query", "error": str(e)}
            )

    def _format_coverage_status(self, coverage_info: Dict[str, Any]) -> str:
        """Format coverage status for prompt (avoid f-string issues)."""
        if not coverage_info or coverage_info["all_covered"]:
            return ""
        missing = ", ".join([e.title() for e in coverage_info["missing"]])
        covered = ", ".join([e.title() for e in coverage_info["covered"]])
        return f"COVERAGE STATUS:\n- âœ… HAVE sources for: {covered}\n- âŒ MISSING sources for: {missing}\n\n"

    def _extract_mentioned_entities(self, query: str) -> List[str]:
        """Extract specific entities (people, concepts) mentioned in query."""
        # Known people in corpus (expand as needed)
        known_people = {
            "phil jackson", "jerrod carmichael", "iÃ±Ã¡rritu", "alejandro inarritu", 
            "rick rubin", "robert greene", "tyler cowen", "marlon brando",
            "will smith", "judd apatow", "whitney cummings", "chris pine",
            "joe dispenza", "carnivore aurelius", "andrew huberman", "david whyte"
        }
        
        query_lower = query.lower()
        entities = []
        
        # Check for known people
        for person in known_people:
            if person in query_lower:
                entities.append(person)
        
        # Pattern: "across X, Y, and Z" or "X, Y, and Z"
        # Pattern: "What do X and Y have in common"
        import re
        across_pattern = r"(?:across|from|among|between)\s+((?:\w+(?:\s+\w+)?(?:\s+and\s+|\s*,\s*))+[\w\s]+)"
        common_pattern = r"(?:common|shared|together).*?([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?(?:\s+and\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)+)"
        
        for pattern in [across_pattern, common_pattern]:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                entity_str = match.group(1).lower()
                # Split by "and", ","
                parts = re.split(r'\s+and\s+|,\s*', entity_str)
                for part in parts:
                    part = part.strip()
                    if part and len(part) > 3:  # Valid name length
                        if part not in entities:
                            entities.append(part)
        
        return entities

    def _validate_entity_coverage(
        self, 
        entities: List[str], 
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Validate that we have sources for all mentioned entities."""
        if not entities:
            return {"all_covered": True, "missing": [], "covered": []}
        
        coverage = {entity: False for entity in entities}
        
        # Check RAG results
        for r in rag_results:
            metadata = r.get("metadata", {})
            episode_id = (metadata.get("episode_id") or "").lower()
            speaker = (metadata.get("speaker") or "").lower()
            text = (r.get("text") or "").lower()
            
            for entity in entities:
                entity_parts = entity.split()
                # Check episode_id (e.g., "002_JERROD_CARMICHAEL")
                if any(part in episode_id for part in entity_parts):
                    coverage[entity] = True
                # Check speaker
                if any(part in speaker for part in entity_parts):
                    coverage[entity] = True
                # Check text mentions
                if entity in text:
                    coverage[entity] = True
        
        # Check KG results (Person nodes)
        for r in kg_results:
            concept = (r.get("concept") or "").lower()
            if r.get("type") == "Person":
                for entity in entities:
                    entity_parts = entity.split()
                    if any(part in concept for part in entity_parts):
                        coverage[entity] = True
        
        covered = [e for e, has_coverage in coverage.items() if has_coverage]
        missing = [e for e, has_coverage in coverage.items() if not has_coverage]
        
        return {
            "all_covered": len(missing) == 0,
            "missing": missing,
            "covered": covered,
        }

    def _handle_count_query(
        self,
        query: str,
    ) -> Optional[AgentResponse]:
        """Handle 'how many' questions by actually querying the data."""
        query_lower = query.lower()
        
        # Check for count questions
        count_patterns = [
            r"how many (?:transcripts?|episodes?)",
            r"how many (?:concepts?|practices?|nodes?)",
            r"what is the (?:total|count|number) of (?:transcripts?|episodes?|concepts?)",
            r"number of (?:transcripts?|episodes?|concepts?)",
        ]
        
        import re
        is_count_query = any(re.search(pattern, query_lower) for pattern in count_patterns)
        
        if not is_count_query:
            return None
        
        # Count transcripts/episodes from Qdrant or Neo4j
        episode_count = 0
        concept_count = 0
        
        # Try to count distinct episodes from Neo4j first (more accurate)
        if self.neo4j_client:
            try:
                # Try to get distinct episode_ids from any node property
                cypher_episodes = """
                MATCH (c)
                WHERE c.workspace_id = $workspace_id
                WITH c.episode_ids as episode_ids
                UNWIND COALESCE(episode_ids, []) as episode_id
                RETURN count(DISTINCT episode_id) as distinct_episodes
                """
                result = self.neo4j_client.execute_read(cypher_episodes, {"workspace_id": self.workspace_id})
                if result and result[0].get("distinct_episodes", 0) > 0:
                    episode_count = result[0].get("distinct_episodes", 0)
            except Exception as e:
                self.logger.warning(f"Could not count episodes from Neo4j: {e}")
        
        # Fallback: Estimate from Qdrant
        if episode_count == 0 and self.hybrid_retriever:
            try:
                qdrant = self.hybrid_retriever.qdrant_client
                collection_name = f"{self.workspace_id}_chunks"
                try:
                    collection_info = qdrant.get_collection(collection_name)
                    total_points = collection_info.points_count
                    # Estimate: ~50-100 chunks per episode on average
                    estimated_episodes = total_points // 75 if total_points > 0 else 0
                    episode_count = estimated_episodes
                except:
                    pass
            except Exception as e:
                self.logger.warning(f"Could not count episodes from Qdrant: {e}")
        
        # Count concepts from Neo4j
        if self.neo4j_client:
            try:
                cypher = """
                MATCH (c)
                WHERE c.workspace_id = $workspace_id
                  AND (c:Concept OR c:Practice OR c:Person OR c:Principle OR c:Outcome 
                       OR c:CognitiveState OR c:BehavioralPattern)
                RETURN count(DISTINCT c) as total
                """
                result = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id})
                if result:
                    concept_count = result[0].get("total", 0)
            except Exception as e:
                self.logger.warning(f"Could not count concepts from Neo4j: {e}")
        
        # Generate answer
        answer_parts = []
        if "transcript" in query_lower or "episode" in query_lower:
            if episode_count > 0:
                answer_parts.append(f"I have access to approximately **{episode_count} podcast episodes** in the knowledge base.")
            else:
                answer_parts.append("I'm processing podcast transcripts, but I don't have an exact count available.")
        
        if "concept" in query_lower or "node" in query_lower or "practice" in query_lower:
            if concept_count > 0:
                answer_parts.append(f"The knowledge graph contains **{concept_count} concepts** (including practices, people, principles, outcomes, and cognitive states).")
            else:
                answer_parts.append("I don't have an exact count of concepts at the moment.")
        
        if not answer_parts:
            return None
        
        return AgentResponse(
            answer=" ".join(answer_parts),
            tools_used=["data_query"],
            metadata={"type": "count_query", "episode_count": episode_count, "concept_count": concept_count}
        )

    def _detect_complex_question(self, query: str) -> Dict[str, Any]:
        """
        Detect if a question is complex (multi-part, ambiguous, or requires clarification).
        
        Returns:
            Dict with 'is_complex', 'reason', and optionally 'sub_questions'
        """
        query_lower = query.lower().strip()
        
        # Patterns indicating multi-part questions
        multi_part_patterns = [
            r'\band\b.*\?',  # "What is X and what about Y?"
            r'\balso\b',     # "Also tell me..."
            r'\bfirst\b.*\bthen\b',  # "First X, then Y"
            r'\b(additionally|furthermore|moreover)\b',
            r'\?.*\?',       # Multiple question marks
        ]
        
        # Check for multiple question indicators
        question_count = query.count('?')
        has_multi_parts = question_count > 1 or any(re.search(p, query_lower) for p in multi_part_patterns)
        
        # Check for vague/ambiguous questions
        vague_patterns = [
            r'^(what|how|why|tell me) about (.{1,10})$',  # Very short "what about X"
            r'^(explain|describe|talk about) (.{1,15})$',
        ]
        is_vague = any(re.search(p, query_lower) for p in vague_patterns)
        
        # Check if it's a comparison question
        comparison_patterns = [
            r'\bcompare\b',
            r'\bdifference between\b',
            r'\bsimilar(ities)?\b.*\bbetween\b',
            r'\bvs\.?\b|\bversus\b',
        ]
        is_comparison = any(re.search(p, query_lower) for p in comparison_patterns)
        
        return {
            "is_complex": has_multi_parts or is_comparison,
            "is_vague": is_vague,
            "has_multiple_parts": has_multi_parts,
            "is_comparison": is_comparison,
            "question_count": question_count,
        }

    def _handle_complex_question(
        self,
        query: str,
        complexity_info: Dict[str, Any],
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> Optional[AgentResponse]:
        """
        Handle complex questions by potentially breaking them down or providing structured responses.
        
        Returns AgentResponse if handled, None if should fall through to normal processing.
        """
        # For comparison questions, ensure we search for all entities
        if complexity_info.get("is_comparison"):
            # Don't block - let normal processing handle it with enhanced search
            return None
        
        # For multi-part questions with 2+ question marks, acknowledge and address each part
        if complexity_info.get("question_count", 0) >= 2:
            # Let the LLM handle multi-part questions naturally
            # The improved prompts should handle this
            return None
        
        return None

    def _handle_knowledge_query(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        """Handle knowledge queries - use BOTH RAG + KG."""
        tools_used = []
        rag_results = []
        kg_results = []
        
        # Check for complex questions
        complexity_info = self._detect_complex_question(query)
        if complexity_info.get("is_complex"):
            self.logger.info(f"Detected complex question: {complexity_info}")
            complex_response = self._handle_complex_question(
                query, complexity_info, conversation_history, session_metadata
            )
            if complex_response:
                return complex_response
        
        # Check if this is a count query first
        count_response = self._handle_count_query(query)
        if count_response:
            return count_response
        
        # Check if user is sharing info about themselves
        self._extract_user_info(query, session_metadata)
        
        # Extract mentioned entities for coverage validation
        mentioned_entities = self._extract_mentioned_entities(query)
        if mentioned_entities:
            self.logger.info(f"Detected mentioned entities: {mentioned_entities}")
        
        # Resolve pronouns
        resolved_query = self._resolve_pronouns(query, session_metadata)
        
        # RAG Search
        if self.hybrid_retriever:
            try:
                rag_results = self.hybrid_retriever.retrieve(resolved_query, use_vector=True, use_graph=False)
                tools_used.append("search_transcripts")
                self.logger.info(f"RAG returned {len(rag_results)} results")
            except Exception as e:
                self.logger.error(f"RAG search failed: {e}")
        
        # KG Search
        if self.neo4j_client:
            try:
                kg_results = self._search_knowledge_graph(resolved_query)
                tools_used.append("search_knowledge_graph")
                self.logger.info(f"KG returned {len(kg_results)} results")
            except Exception as e:
                self.logger.error(f"KG search failed: {e}")
        
        # Validate entity coverage for multi-entity queries
        coverage_info = None
        if mentioned_entities and len(mentioned_entities) > 1:
            coverage_info = self._validate_entity_coverage(mentioned_entities, rag_results, kg_results)
            self.logger.info(f"Entity coverage: {coverage_info}")
            
            # Filter RAG results to ONLY include mentioned entities (strict filtering)
            filtered_rag = []
            for r in rag_results:
                metadata = r.get("metadata", {})
                episode_id = (metadata.get("episode_id") or "").lower()
                speaker = (metadata.get("speaker") or "").lower()
                text = (r.get("text") or "").lower()
                
                # Check if this result is from one of the mentioned entities
                is_from_mentioned = False
                for entity in mentioned_entities:
                    entity_parts = entity.split()
                    # Check episode_id (e.g., "002_JERROD_CARMICHAEL")
                    if any(part in episode_id for part in entity_parts):
                        is_from_mentioned = True
                        break
                    # Check speaker
                    if any(part in speaker for part in entity_parts):
                        is_from_mentioned = True
                        break
                
                if is_from_mentioned:
                    filtered_rag.append(r)
            
            # Use filtered results (only if we have some)
            if filtered_rag:
                self.logger.info(f"Filtered RAG results: {len(rag_results)} â†’ {len(filtered_rag)} (only mentioned entities)")
                rag_results = filtered_rag
            else:
                self.logger.warning(f"No RAG results match mentioned entities: {mentioned_entities}")
            
            if not coverage_info["all_covered"]:
                missing = coverage_info["missing"]
                self.logger.warning(f"Missing coverage for entities: {missing}")
        
        # No results
        if not rag_results and not kg_results:
            return AgentResponse(
                answer="I couldn't find information about that in the podcast knowledge base. Could you rephrase or ask about a different topic related to philosophy, creativity, coaching, or personal development?",
                tools_used=tools_used,
                metadata={"type": "no_results"}
            )
        
        # Synthesize answer (pass coverage info for strict validation)
        answer = self._synthesize_answer(
            query, resolved_query, rag_results[:5], kg_results[:10], 
            conversation_history, coverage_info=coverage_info, mentioned_entities=mentioned_entities
        )
        
        # Extract sources with full metadata
        sources = self._extract_sources(rag_results[:5], kg_results[:10])
        
        # Log sources for debugging
        self.logger.info(f"Extracted {len(sources)} sources", extra={
            "sample_source": sources[0] if sources else None,
            "rag_sample_metadata": rag_results[0].get("metadata", {}) if rag_results else None
        })
        
        # Update active entity
        self._update_active_entity(query, rag_results, kg_results, session_metadata)
        
        return AgentResponse(
            answer=answer,
            tools_used=tools_used,
            sources=sources,
            metadata={
                "type": "knowledge_query",
                "rag_count": len(rag_results),
                "kg_count": len(kg_results),
            }
        )

    def _resolve_pronouns(self, query: str, session_metadata: Optional[Dict[str, Any]] = None) -> str:
        """Resolve pronouns using active entity."""
        if not session_metadata:
            return query
        
        active_entity = session_metadata.get("active_entity")
        if not active_entity:
            return query
        
        query_lower = query.lower()
        pronouns = ["he", "she", "they", "him", "her", "them", "his", "her", "their"]
        
        for pronoun in pronouns:
            if f" {pronoun} " in f" {query_lower} " or query_lower.startswith(f"{pronoun} "):
                return f"{query} (referring to {active_entity})"
        
        return query

    def _search_knowledge_graph(self, query: str) -> List[Dict[str, Any]]:
        """Search KG for relevant concepts."""
        results = []
        words = query.lower().split()
        
        cypher = """
        MATCH (c)
        WHERE c.workspace_id = $workspace_id
          AND (toLower(c.name) CONTAINS $search_term 
               OR toLower(c.description) CONTAINS $search_term)
        OPTIONAL MATCH (c)-[r]->(related)
        WHERE related.workspace_id = $workspace_id
        RETURN c.name as concept, 
               labels(c)[0] as type,
               c.description as description,
               collect(DISTINCT {rel: type(r), target: related.name})[..5] as relationships
        LIMIT 10
        """
        
        search_terms = [query.lower()] + [w for w in words if len(w) > 3]
        
        for term in search_terms[:3]:
            try:
                result = self.neo4j_client.execute_read(cypher, {"workspace_id": self.workspace_id, "search_term": term})
                if result:
                    results.extend(result)
            except Exception as e:
                self.logger.warning(f"KG search for '{term}' failed: {e}")
        
        # Deduplicate
        seen = set()
        unique = []
        for r in results:
            concept = r.get("concept", "")
            if concept and concept not in seen:
                seen.add(concept)
                unique.append(r)
        
        return unique

    def _synthesize_answer(
        self,
        query: str,
        resolved_query: str,
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        coverage_info: Optional[Dict[str, Any]] = None,
        mentioned_entities: Optional[List[str]] = None,
    ) -> str:
        """Synthesize answer from RAG + KG using LLM with strict entity coverage."""
        import os
        
        # Format RAG context with full metadata
        rag_context = ""
        if rag_results:
            rag_parts = []
            for i, r in enumerate(rag_results, 1):
                text = r.get("text", "")[:400]
                metadata = r.get("metadata", {})
                
                # Extract episode info
                episode_id = metadata.get("episode_id", "")
                source_path = metadata.get("source_path", "")
                if not episode_id or episode_id == "unknown":
                    if source_path:
                        filename = os.path.basename(source_path)
                        episode_id = os.path.splitext(filename)[0]
                
                speaker = metadata.get("speaker", "")
                timestamp = metadata.get("timestamp", "")
                
                # Format source line - emphasize speaker and episode for natural citation
                # Parse episode name to be more readable (e.g., "143_TYLER_COWEN_PART_1" -> "Tyler Cowen")
                speaker_display = speaker if speaker and speaker != "Unknown" else ""
                episode_display = ""
                
                if episode_id and episode_id != "unknown":
                    # Extract name from episode_id like "143_TYLER_COWEN_PART_1"
                    parts = episode_id.split("_")
                    if len(parts) >= 2:
                        # Skip number prefix and "PART" suffix
                        name_parts = [p for p in parts[1:] if p.upper() not in ["PART", "1", "2", "3"]]
                        if name_parts:
                            episode_display = " ".join(name_parts).title()
                    if not episode_display:
                        episode_display = episode_id.replace("_", " ")
                
                # Use episode name as speaker if speaker is generic
                if not speaker_display or speaker_display in ["Speaker 1", "Speaker 2", "Speaker"]:
                    speaker_display = episode_display or "Speaker"
                
                source_info = f"--- {speaker_display}"
                if episode_id and episode_id != "unknown":
                    source_info += f" (Episode: {episode_id})"
                if timestamp:
                    source_info += f" at {timestamp}"
                source_info += " ---"
                
                rag_parts.append(f"{source_info}\n\"{text}\"")
            rag_context = "\n\n".join(rag_parts)
        
        # Format KG context
        kg_context = ""
        if kg_results:
            kg_parts = []
            for r in kg_results:
                concept = r.get("concept", "")
                desc = r.get("description", "")
                rels = r.get("relationships", [])
                
                part = f"**{concept}**"
                if desc:
                    part += f": {desc[:200]}"
                if rels:
                    rel_strs = [f"{rel['rel']} â†’ {rel['target']}" for rel in rels[:3]]
                    part += f" (Relationships: {', '.join(rel_strs)})"
                kg_parts.append(part)
            kg_context = "\n".join(kg_parts)
        
        # Conversation context
        conv_context = ""
        if conversation_history:
            recent = conversation_history[-5:]
            conv_parts = [f"{m.get('role', 'user')}: {m.get('content', '')[:200]}" for m in recent]
            conv_context = "\n".join(conv_parts)
        
        # Build entity coverage constraint for multi-entity queries
        entity_coverage_constraint = ""
        if mentioned_entities and len(mentioned_entities) > 1:
            mentioned_list = ', '.join([e.title() for e in mentioned_entities])
            
            if coverage_info and not coverage_info["all_covered"]:
                missing = ", ".join([e.title() for e in coverage_info["missing"]])
                covered = ", ".join([e.title() for e in coverage_info["covered"]])
            else:
                missing = ""
                covered = mentioned_list
            
            entity_coverage_constraint = f"""
CRITICAL ENTITY COVERAGE REQUIREMENT (MULTI-ENTITY QUERY):
The user specifically asked about these people: {mentioned_list}

STRICT RULES - YOU MUST FOLLOW:
1. ONLY use sources from: {mentioned_list}
2. DO NOT use sources from ANYONE else (e.g., Judd Apatow, Rick Rubin, Edward Norton, etc.)
3. If you don't have sources for ALL mentioned entities, be explicit: "I don't have sufficient sources from [entity]"
4. DO NOT use proxy speakers, "peers", "similar creatives", or related people as substitutes
5. DO NOT infer shared principles if you don't have direct evidence from ALL mentioned entities

ALLOWED SPEAKERS FOR THIS QUERY: {mentioned_list}
FORBIDDEN: Any speaker NOT in the list above (even if they appear in the sources below)

{self._format_coverage_status(coverage_info) if coverage_info and not coverage_info["all_covered"] else ''}
EXAMPLE HONEST RESPONSE IF MISSING COVERAGE:
"For Phil Jackson and IÃ±Ã¡rritu, [principle]. However, I don't have sufficient sources from Jerrod Carmichael in the knowledge base to confirm this principle. Therefore, I can only establish partial overlap."

REMEMBER: Even if Judd Apatow or Rick Rubin appear in the sources, DO NOT use them - they are NOT in the query!"""
        
        # Synthesize with LLM
        system_prompt = f"""You are Sage, a warm and intellectually curious Podcast Intelligence Assistant. You help people explore insights from fascinating podcast conversations.
{entity_coverage_constraint}
PERSONALITY:
- Genuinely excited about sharing insights and making connections
- Speak like a thoughtful friend who's passionate about ideas
- Weave in context naturally, like you're having a conversation
- Show enthusiasm for the ideas you're sharing
- If you remember the user's name or previous context, reference it naturally

CRITICAL RULES FOR ACCURACY:
1. ONLY cite and reference speakers/episodes that appear in the TRANSCRIPT SOURCES below
2. DO NOT mention or cite anyone not in the provided sources
3. If a speaker is not in the sources, DO NOT reference them - state this explicitly
4. Cite naturally by speaker name (e.g., "Phil Jackson shares this fascinating insight..." not "[Source 1]")

RESPONSE STYLE:
- Start with the insight, not meta-commentary
- Weave citations naturally into your narrative
- Show genuine enthusiasm for interesting ideas
- Make connections between concepts when relevant
- End with an invitation to explore further if appropriate

WHAT YOU CANNOT DO:
- Reference speakers not in the provided sources
- Make up quotes or information
- Use "[Source 1]" style citations
- Infer shared principles without direct evidence

FORMATTING:
- Use plain paragraphs for most content
- For lists, use proper markdown with "- " prefix
- Keep formatting simple and clean"""

        user_prompt = f"""Question: {query}

TRANSCRIPT SOURCES (cite by speaker name and episode, NOT by source number):
{rag_context if rag_context else "No relevant transcripts found."}

KNOWLEDGE GRAPH:
{kg_context if kg_context else "No relevant concepts found."}

{f"CONVERSATION CONTEXT:{chr(10)}{conv_context}" if conv_context else ""}

Provide a well-sourced answer with NATURAL citations. {f"IMPORTANT: Be honest about missing coverage for any entities." if coverage_info and not coverage_info["all_covered"] else ""}"""

        try:
            # Build proper messages array with conversation history for follow-up support
            messages = [{"role": "system", "content": system_prompt}]
            
            # Add conversation history as proper messages (enables better follow-up)
            if conversation_history:
                for msg in conversation_history[-5:]:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content:
                        messages.append({"role": role, "content": content})
            
            # Add current query with context
            messages.append({"role": "user", "content": user_prompt})
            
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.5,
                max_tokens=800,
            )
            return response.choices[0].message.content
        except Exception as e:
            self.logger.error(f"Synthesis failed: {e}")
            return f"Here's what I found:\n\n{rag_context[:500] if rag_context else kg_context[:500]}"

    def _extract_sources(
        self,
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Extract sources with FULL metadata."""
        import os
        sources = []
        seen = set()
        
        for r in rag_results:
            metadata = r.get("metadata", {})
            
            # Try multiple possible field names for episode_id
            episode_id = (
                metadata.get("episode_id") or 
                metadata.get("episode") or 
                metadata.get("file_name") or
                metadata.get("filename") or
                ""
            )
            
            source_path = metadata.get("source_path") or metadata.get("source") or metadata.get("path") or ""
            
            # Try multiple possible field names for speaker
            speaker = (
                metadata.get("speaker") or 
                metadata.get("speaker_name") or
                metadata.get("author") or
                ""
            )
            
            # Try multiple possible field names for timestamp
            timestamp = (
                metadata.get("timestamp") or 
                metadata.get("start_time") or
                metadata.get("time") or
                ""
            )
            
            text = r.get("text", "")[:200]
            
            # Extract episode from path if needed
            if not episode_id or episode_id == "unknown":
                if source_path:
                    filename = os.path.basename(source_path)
                    episode_id = os.path.splitext(filename)[0]
            
            # Log what we found for debugging
            self.logger.debug(f"Source extraction: episode_id={episode_id}, speaker={speaker}, metadata_keys={list(metadata.keys())}")
            
            # Deduplicate by episode + speaker + timestamp
            key = f"{episode_id}:{speaker}:{timestamp}"
            if key not in seen:
                seen.add(key)
                sources.append({
                    "type": "transcript",
                    "episode_id": episode_id or "Unknown Episode",
                    "speaker": speaker or "Unknown Speaker",
                    "timestamp": timestamp or "",
                    "text": text,
                    "source_path": source_path,
                })
        
        for r in kg_results:
            concept = r.get("concept", "")
            if concept:
                sources.append({
                    "type": "knowledge_graph",
                    "concept": concept,
                    "node_type": r.get("type", "Concept"),
                    "description": r.get("description", "")[:100],
                })
        
        return sources

    def _update_active_entity(
        self,
        query: str,
        rag_results: List[Dict[str, Any]],
        kg_results: List[Dict[str, Any]],
        session_metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Update active entity for follow-up questions."""
        if not session_metadata:
            return
        
        # Extract entity from query
        patterns = [
            r"who is ([a-zA-Z\s]+)\??",
            r"what does ([a-zA-Z\s]+) (?:say|think|believe)",
            r"tell me about ([a-zA-Z\s]+)",
            r"([A-Z][a-z]+ [A-Z][a-z]+)",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                session_metadata["active_entity"] = match.group(1).strip().title()
                return
        
        # Or from KG results
        for r in kg_results:
            if r.get("type") == "Person":
                session_metadata["active_entity"] = r.get("concept")
                return
